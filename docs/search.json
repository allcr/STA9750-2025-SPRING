[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA9750 Spring 2025 Submission Material",
    "section": "",
    "text": "Last Updated:   2025-03-26 at 20:43PM  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreen Transit Alliance for Investigation of Variance Annual Awards (Mini Project 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNostalgia Exploration (Mini Project 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 4\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "",
    "text": "In February 2025, the Commission to Analyze Taxpayer Spending was asked to analyze several questions of interest to the Mayor of New York City and the New York City Council regarding the budget for the City. In this document, we will present our findings to each of these questions."
  },
  {
    "objectID": "mp01.html#which-job-title-has-the-highest-base-rate-of-pay",
    "href": "mp01.html#which-job-title-has-the-highest-base-rate-of-pay",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "Which job title has the highest base rate of pay?",
    "text": "Which job title has the highest base rate of pay?\nThe Commission has been asked which role within New York City has the highest base rate of pay. We will be using the annualized base rate of pay and not the gross annual pay for this analysis.\n\n\nCode\nq1_data &lt;- cleaned |&gt;\n  group_by(title_description) |&gt;\n  summarize(avg_base = mean(base_annual_pay)) |&gt;\n  slice_max(order_by = avg_base, n = 1)\n\nkable(q1_data |&gt;\n  rename(\n    \"Title / Job Description\" = title_description,\n    \"Average Base Annual Pay (USD)\" = avg_base\n  ), digits = 0, format.args = list(big.mark = \",\"))\n\n\n\n\n\nTitle / Job Description\nAverage Base Annual Pay (USD)\n\n\n\n\nChief Actuary\n296,470\n\n\n\n\n\nThe job title with the highest average annual base pay is Chief Actuary at $296,470 per year."
  },
  {
    "objectID": "mp01.html#what-individual-and-in-what-year-had-the-highest-total-pay",
    "href": "mp01.html#what-individual-and-in-what-year-had-the-highest-total-pay",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "What individual and in what year had the highest total pay?",
    "text": "What individual and in what year had the highest total pay?\nThe Commission has decided to review this using the total gross pay as calculated by the methodology within the data dictionary provided by the city and not the annualized base rate of pay.\n\n\nCode\nq2_data &lt;- cleaned |&gt;\n  select(fiscal_year, agency_name, title_description, last_name, first_name, mid_init, gross_annual_pay) |&gt;\n  ungroup() |&gt;\n  slice_max(gross_annual_pay, n = 1)\n\n\nMark K. Tettonis, the Chief Marine Engineer of the Department Of Transportation had the highest total pay of $1,689,518 in 2024."
  },
  {
    "objectID": "mp01.html#which-individual-had-worked-the-most-overtime-hours",
    "href": "mp01.html#which-individual-had-worked-the-most-overtime-hours",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "Which individual had worked the most overtime hours?",
    "text": "Which individual had worked the most overtime hours?\n\nHighest Logged Amount of Overtime Hours Annually\nThe Commission has found that the below employees have logged the largest amount of overtime hours per year. The Police Department notably is featured several times on this list.\nThis analysis is weakened by the lack of a unique employee identifier within the data. We have found many cases where common names are repeated throughout the employee payroll data. Specifically, the abbreviation “MD” for Muhammad and the surname Islam. Muhammad is one of the world’s most popular names and it appears often in the payroll data, specifically with the surname Islam. We are unable to uniquely identify the individuals and can only drill down to the name level.\n\n\nCode\ncleaned |&gt;\n  group_by(fiscal_year, agency_name, last_name, first_name, mid_init, title_description) |&gt;\n  summarize(total_ot_hours = sum(ot_hours)) |&gt;\n  ungroup(agency_name, last_name, first_name, mid_init, title_description) |&gt;\n  slice_max(total_ot_hours, n = 1) |&gt;\n  mutate(total_ot_days = total_ot_hours / 24) |&gt;\n  arrange(desc(fiscal_year)) |&gt;\n  rename(\n    \"Fiscal Year\" = fiscal_year,\n    \"Total Overtime Hours\" = total_ot_hours,\n    \"Total Overtime in Days\" = total_ot_days,\n    \"Title / Job Description\" = title_description,\n    \"Agency Name\" = agency_name,\n    \"First Name\" = first_name,\n    \"Last Name\" = last_name,\n    \"Middle Initial\" = mid_init\n  ) |&gt;\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiscal Year\nAgency Name\nLast Name\nFirst Name\nMiddle Initial\nTitle / Job Description\nTotal Overtime Hours\nTotal Overtime in Days\n\n\n\n\n2024\nHra/Dept Of Social Services\nLeslie\nAsalay\nNA\nBenefits Opportunity Specialist\n3702.00\n154.2500\n\n\n2023\nDepartment Of Correction\nCastillo\nJohn\nNA\nCorrection Officer\n3943.05\n164.2937\n\n\n2022\nFire Department\nShelley\nJames\nM\nWiper\n5612.00\n233.8333\n\n\n2021\nDepartment Of Sanitation\nSarno\nJohn\nM\nSupervisor\n3131.00\n130.4583\n\n\n2020\nNyc Housing Authority\nDavis\nKenny\nA\nMaintenance Worker\n5051.00\n210.4583\n\n\n2019\nPolice Department\nIslam\nMd\nS\nTraffic Enforcement Agent Al 1 & 2 Only\n3145.00\n131.0417\n\n\n2018\nPolice Department\nIslam\nMd\nS\nTraffic Enforcement Agent Al 1 & 2 Only\n3028.50\n126.1875\n\n\n2017\nDepartment Of Correction\nCastillo\nJohn\nNA\nCorrection Officer\n2935.75\n122.3229\n\n\n2016\nPolice Department\nIslam\nMd\nS\nTraffic Enforcement Agent\n2987.50\n124.4792\n\n\n2015\nPolice Department\nUddin\nMohammad\nN\nTraffic Enforcement Agent\n5308.50\n221.1875\n\n\n2014\nDepartment Of Buildings\nMurphy\nJohn\nNA\nAssociate Inspector\n3347.50\n139.4792\n\n\n\n\n\n\n\nEmployee with Highest Total Amount of Logged Overtime Hours\n\n\nCode\ncleaned |&gt;\n  group_by(agency_name, last_name, first_name, mid_init, title_description) |&gt;\n  summarize(total_ot_hours = sum(ot_hours)) |&gt;\n  ungroup() |&gt;\n  slice_max(total_ot_hours, n = 1) |&gt;\n  mutate(total_ot_days = total_ot_hours / 24) |&gt;\n  rename(\n    \"Title / Job Description\" = title_description,\n    \"Agency Name\" = agency_name,\n    \"First Name\" = first_name,\n    \"Last Name\" = last_name,\n    \"Middle Initial\" = mid_init,\n    \"Total Overtime Hours\" = total_ot_hours,\n    \"Total Overtime in Days\" = total_ot_days\n  ) |&gt;\n  kable(digits = 2, format.args = list(big.mark = \",\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgency Name\nLast Name\nFirst Name\nMiddle Initial\nTitle / Job Description\nTotal Overtime Hours\nTotal Overtime in Days\n\n\n\n\nDepartment Of Correction\nCastillo\nJohn\nNA\nCorrection Officer\n22,119.74\n921.66\n\n\n\n\n\nJohn Castillo, a Correctional Officer in the Department of Corrections, has logged the largest amount of overtime hours within NYC. Mr. Castillo has logged 22,120 hours or 922 days of overtime according to publicly available data."
  },
  {
    "objectID": "mp01.html#which-agency-has-the-highest-average-total-annual-gross-payroll",
    "href": "mp01.html#which-agency-has-the-highest-average-total-annual-gross-payroll",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "Which agency has the highest average total annual gross payroll?",
    "text": "Which agency has the highest average total annual gross payroll?\n\n\nCode\ncleaned |&gt;\n  group_by(aggregated_agency_name) |&gt;\n  summarize(mean_annual_gross_salary = mean(gross_annual_pay)) |&gt;\n  slice_max(mean_annual_gross_salary, n = 10) |&gt;\n  rename(\n    \"Agency Name (Aggregated)\" = aggregated_agency_name,\n    \"Mean Annual Gross Salary (USD)\" = mean_annual_gross_salary\n  ) |&gt;\n  kable(digits = 2, format.args = list(big.mark = \",\"))\n\n\n\n\n\nAgency Name (Aggregated)\nMean Annual Gross Salary (USD)\n\n\n\n\nOffice Of Collective Bargainin\n105,562.73\n\n\nFinancial Info Svcs Agency\n105,436.71\n\n\nFire Department\n100,771.68\n\n\nOffice Of The Actuary\n98,543.01\n\n\nMunicipal Water Fin Authority\n92,880.93\n\n\nDepartment Of Correction\n89,654.30\n\n\nNyc Fire Pension Fund\n88,934.29\n\n\nIndependent Budget Office\n85,512.45\n\n\nPolice Department\n84,564.47\n\n\nConflicts Of Interest Board\n84,526.45\n\n\n\n\n\nThe Office of Collective Bargaining has the highest average gross salary of any agency within New York City. This conclusion is sound as its stated mission1 is to help the City negotiate collective bargaining with unions and the department will need a high number of attorneys on payroll to do so."
  },
  {
    "objectID": "mp01.html#which-agency-has-the-most-employees-on-payroll-each-year",
    "href": "mp01.html#which-agency-has-the-most-employees-on-payroll-each-year",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "Which agency has the most employees on payroll each year?",
    "text": "Which agency has the most employees on payroll each year?\n\n\nCode\nq5 &lt;- cleaned |&gt;\n  group_by(fiscal_year, aggregated_agency_name) |&gt;\n  summarize(count_employee = n()) |&gt;\n  ungroup() |&gt;\n  slice_max(count_employee, n = 10) |&gt;\n  arrange(desc(fiscal_year)) |&gt;\n  rename(\n    \"Number of Employees on File\" = count_employee,\n    \"Agency Name (Aggregated)\" = aggregated_agency_name,\n    \"Fiscal Year\" = fiscal_year\n  )\nq5$`Number of Employees on File` &lt;- format(q5$`Number of Employees on File`, big.mark = \",\")\n\nkable(q5)\n\n\n\n\n\nFiscal Year\nAgency Name (Aggregated)\nNumber of Employees on File\n\n\n\n\n2024\nDepartment Of Education\n295,223\n\n\n2023\nDepartment Of Education\n289,856\n\n\n2022\nDepartment Of Education\n342,860\n\n\n2021\nDepartment Of Education\n303,807\n\n\n2020\nDepartment Of Education\n313,471\n\n\n2019\nDepartment Of Education\n310,097\n\n\n2018\nDepartment Of Education\n273,855\n\n\n2017\nDepartment Of Education\n266,499\n\n\n2016\nDepartment Of Education\n265,291\n\n\n2015\nDepartment Of Education\n299,815\n\n\n\n\n\nThe Department of Education has the most employees of any department within New York City for all years with publicly available data."
  },
  {
    "objectID": "mp01.html#which-agency-has-the-highest-average-overtime-usage",
    "href": "mp01.html#which-agency-has-the-highest-average-overtime-usage",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "Which agency has the highest average overtime usage?",
    "text": "Which agency has the highest average overtime usage?\n\n\nCode\ncleaned |&gt;\n  group_by(agency_name) |&gt;\n  summarize(avg_ot_count = mean(ot_hours), avg_reg_hours = mean(regular_hours)) |&gt;\n  filter(avg_ot_count != 0) |&gt;\n  arrange(desc(avg_ot_count)) |&gt;\n  slice_max(avg_ot_count, n = 10) |&gt;\n  rename(\n    \"Agency Name\" = agency_name,\n    \"Average Amount of Overtime Hours\" = avg_ot_count,\n    \"Average Amount of Regular Hours\" = avg_reg_hours\n  ) |&gt;\n  kable(digits = 0, format.args = list(big.mark = \",\"))\n\n\n\n\n\n\n\n\n\n\nAgency Name\nAverage Amount of Overtime Hours\nAverage Amount of Regular Hours\n\n\n\n\nFire Department\n347\n1,822\n\n\nDepartment Of Correction\n321\n1,739\n\n\nBoard Of Election\n262\n1,314\n\n\nDepartment Of Sanitation\n224\n1,618\n\n\nPolice Department\n214\n1,717\n\n\nDept Of Citywide Admin Svcs\n189\n1,582\n\n\nDepartment Of Transportation\n185\n1,609\n\n\nNyc Housing Authority\n169\n1,600\n\n\nDept. Of Homeless Services\n164\n1,501\n\n\nAdmin For Children’s Svcs\n121\n1,491\n\n\n\n\n\nThe New York Fire Department has the highest average overtime hours of any department at 347 hours. This was determined by taking the average amount of overtime hours logged by agency. The top ten agencies by average amount of overtime worked is provided above."
  },
  {
    "objectID": "mp01.html#what-is-the-average-salary-of-employees-outside-of-the-five-bouroughs",
    "href": "mp01.html#what-is-the-average-salary-of-employees-outside-of-the-five-bouroughs",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "What is the average salary of employees outside of the five bouroughs?",
    "text": "What is the average salary of employees outside of the five bouroughs?\nThe average salary of employees outside of the five boroughs is listed below. The Commission is limited as the there is a catch all category called “Other”. Further granularity is needed for future analysis. We leave out any employee data where the work location was recorded as NA.\n\n\nCode\ncleaned |&gt;\n  filter(work_location_borough %in% c(\"Albany\", \"Other\", \"Orange\")) |&gt;\n  group_by(work_location_borough) |&gt;\n  summarize(avg_gross_annual_pay = mean(gross_annual_pay)) |&gt;\n  rename(\n    \"Work Location (Borough)\" = work_location_borough,\n    \"Average Gross Annual Pay (USD)\" = avg_gross_annual_pay\n  ) |&gt;\n  kable(digits = 0, format.args = list(big.mark = \",\"))\n\n\n\n\n\nWork Location (Borough)\nAverage Gross Annual Pay (USD)\n\n\n\n\nAlbany\n86,341\n\n\nOrange\n53,821\n\n\nOther\n68,753"
  },
  {
    "objectID": "mp01.html#how-much-has-the-citys-aggregate-payroll-grown-in-the-past-10-years",
    "href": "mp01.html#how-much-has-the-citys-aggregate-payroll-grown-in-the-past-10-years",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "How much has the city’s aggregate payroll grown in the past 10 years?",
    "text": "How much has the city’s aggregate payroll grown in the past 10 years?\n\n\nCode\npay_data &lt;- cleaned |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(total_payroll = sum(gross_annual_pay))\n\nggplot(data = pay_data, aes(x = fiscal_year, y = total_payroll)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  scale_y_continuous(\n    name = \"Payroll of New York City (USD)\",\n    labels = label_currency(scale_cut = cut_short_scale()),\n    breaks = scales::pretty_breaks(n = 6)\n  ) +\n  scale_x_continuous(\n    name = \"Fiscal Year\",\n    breaks = scales::pretty_breaks(n = 10)\n  ) +\n  coord_cartesian(ylim = c(20000000000, NA))\n\n\n\n\n\nNYC Total Payroll per Year\n\n\n\n\n\n\nCode\npay_2014 &lt;- pay_data |&gt;\n  filter(fiscal_year == 2014) |&gt;\n  select(total_payroll)\npay_2024 &lt;- pay_data |&gt;\n  filter(fiscal_year == 2024) |&gt;\n  select(total_payroll)\n\npay_diff &lt;- pay_2024 - pay_2014\n\n\nOver the last ten years, the total base annual payroll of New York City has increased by $9,261,070,771, from $22,854,720,601in 2014 to $32,115,791,372 in 2024."
  },
  {
    "objectID": "mp01.html#proposal-1-introduce-cap-based-on-the-mayors-salary",
    "href": "mp01.html#proposal-1-introduce-cap-based-on-the-mayors-salary",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "Proposal 1: Introduce cap based on the Mayor’s salary",
    "text": "Proposal 1: Introduce cap based on the Mayor’s salary\nWe will review the proposal to make cap all salaries within the city at the pay rate of the mayor. It will be helpful to first see the annual gross salary paid to the currency mayor Eric L. Adams as he is present in the data for multiple years and has held multiple jobs within the city.\n\n\nCode\neric_adams_data &lt;- cleaned |&gt;\n  filter(last_name == \"Adams\", first_name == \"Eric\", mid_init == \"L\")\n\neric_adams_data |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(sum_salary = sum(gross_annual_pay)) |&gt;\n  ggplot(mapping = aes(x = fiscal_year, y = sum_salary)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  scale_y_continuous(\n    name = \"Annual Salary of Eric Adams (USD)\",\n    labels = label_currency(scale_cut = cut_short_scale()),\n    breaks = scales::pretty_breaks(n = 6)\n  ) +\n  scale_x_continuous(\n    name = \"Fiscal Year\",\n    breaks = scales::pretty_breaks(n = 10)\n  )\n\n\n\n\n\nEric L. Adams Payroll per Year\n\n\n\n\nEric Adams was elected as mayor in the 2021 election. It is important to note that his pay for 2022 records his (now) current pay as well as the pay for his previous role as the Brooklyn Borough President. We do see a marked increase in his annual gross pay from 2015 to 2024.\nAs defined in the New York City Charter2, the mayor is entitled to a salary of $258,750 per year. We will use this value as the maximum gross annual pay for all employees.\n\n\nCode\nmayor_cap_2023 &lt;- cleaned |&gt;\n  filter(fiscal_year == 2023) |&gt;\n  mutate(gross_annual_pay = case_when(\n    gross_annual_pay &gt; 258750 ~ 258750,\n    TRUE ~ gross_annual_pay\n  )) |&gt;\n  select(gross_annual_pay) |&gt;\n  sum()\n\n\nBy capping the total gross annual pay to match that of the mayor, the expected payroll would have been $31,057,950,855, representing a savings of $11,676,718.\nThis value is not as high as we the Commission had hoped. Let us see what the distribution of gross annual pay for the year 2023 looks like.\n\n\nCode\ncleaned |&gt;\n  filter(fiscal_year == 2023) |&gt;\n  select(pay_basis, gross_annual_pay) |&gt;\n  ggplot(aes(x = gross_annual_pay)) +\n  geom_histogram() +\n  scale_y_continuous(\n    name = \"Count of Employees at Salary Level USD\",\n    breaks = scales::pretty_breaks(n = 6)\n  ) +\n  scale_x_continuous(\n    name = \"Gross Annual Pay\",\n    labels = label_currency(scale_cut = cut_short_scale()),\n    breaks = scales::pretty_breaks(n = 10)\n  ) +\n  geom_vline(aes(xintercept = 258750),\n    color = \"blue\", linetype = \"dashed\", size = 1\n  )\n\n\n\n\n\nHistogram of Payroll for Fiscal Year 2023\n\n\n\n\nHere is the distribution of all employee pay for 2023, with the mayoral salary indicated with the blue line. We can see that there are significantly few people making more than the mayoral salary compared to those making less than the mayoral salary. This includes all employee types. Let us limit our analysis to only the annually salaried city employees.\n\n\nCode\ncleaned |&gt;\n  filter(\n    fiscal_year == 2023,\n    pay_basis %in% c(\"Per Annum\", \"Prorated Annual\")\n  ) |&gt;\n  select(pay_basis, gross_annual_pay) |&gt;\n  ggplot(aes(x = gross_annual_pay)) +\n  geom_histogram() +\n  scale_y_continuous(\n    name = \"Count of Employees at Annual Salary Level\",\n    breaks = scales::pretty_breaks(n = 6)\n  ) +\n  scale_x_continuous(\n    name = \"Gross Annual Pay\",\n    labels = label_currency(scale_cut = cut_short_scale()),\n    breaks = scales::pretty_breaks(n = 10)\n  ) +\n  geom_vline(aes(xintercept = 258750),\n    color = \"blue\", linetype = \"dashed\", size = 1\n  )\n\n\n\n\n\nHistogram of Payroll for annually Salaried for Fiscal Year 2023\n\n\n\n\nWhen restricting our view to annually salaried employees, we can see the limits of the proposal. There are significantly more people making less than the Mayor’s salary compared to the number of people making more than the mayor. This proposal has limited impact, but can be implemented without causing significant disruption."
  },
  {
    "objectID": "mp01.html#proposal-2-increase-regular-capacity-by-hiring-more-employees-to-reduce-overtime-expenditure",
    "href": "mp01.html#proposal-2-increase-regular-capacity-by-hiring-more-employees-to-reduce-overtime-expenditure",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "Proposal 2: Increase regular capacity by hiring more employees to reduce overtime expenditure",
    "text": "Proposal 2: Increase regular capacity by hiring more employees to reduce overtime expenditure\nWe will use a 2000 hour threshold for hiring one more employee, an eight hour work day for 250 business days with 2023 as our benchmark. We will be using the annualized base pay and the total overtime paid for this analysis and not the total gross pay.\n\n\nCode\np2 &lt;- cleaned |&gt;\n  filter(fiscal_year == 2023) |&gt;\n  group_by(agency_name, title_description) |&gt;\n  summarize(\n    average_base_annual_pay = mean(base_annual_pay),\n    total_base_annual_pay = sum(base_annual_pay),\n    total_over_time_paid = sum(total_ot_paid),\n    total_ot_hours = sum(ot_hours)\n  ) |&gt;\n  mutate(\n    number_new_full_time_employees = total_ot_hours %/% 2000,\n    new_employee_cost = average_base_annual_pay * number_new_full_time_employees,\n    new_employee_savings = total_over_time_paid - new_employee_cost,\n    is_new_employee_outright_cheaper = case_when(\n      number_new_full_time_employees == 0 ~ 0,\n      total_over_time_paid &gt; new_employee_cost ~ 1,\n      TRUE ~ 0\n    )\n  )\n\ntotal_savings &lt;- sum(p2$new_employee_savings, na.rm = TRUE)\n\n\np2 |&gt;\n  group_by(agency_name, title_description, total_ot_hours) |&gt;\n  summarise(\n    total_savings = sum(new_employee_savings, na.rm = TRUE),\n    total_number_new_employees = sum(number_new_full_time_employees, na.rm = TRUE)\n  ) |&gt;\n  filter(total_number_new_employees != 0) |&gt;\n  ungroup() |&gt;\n  slice_max(total_savings, n = 10) |&gt;\n  rename(\n    \"Agency Name\" = agency_name,\n    \"Total Savings (USD)\" = total_savings,\n    \"Total Overtime Hours\" = total_ot_hours,\n    \"Title / Job Description)\" = title_description,\n    \"Number of New Employees\" = total_number_new_employees\n  ) |&gt;\n  kable(format.args = list(big.mark = \",\"))\n\n\n\n\n\n\n\n\n\n\n\n\nAgency Name\nTitle / Job Description)\nTotal Overtime Hours\nTotal Savings (USD)\nNumber of New Employees\n\n\n\n\nFire Department\nFirefighter\n4,302,288.7\n112,235,965\n2,151\n\n\nPolice Department\nPolice Officer\n6,059,176.5\n99,588,694\n3,029\n\n\nDepartment Of Correction\nCorrection Officer\n3,276,266.9\n83,813,354\n1,638\n\n\nDepartment Of Sanitation\nSanitation Worker\n1,586,504.4\n64,525,727\n793\n\n\nPolice Department\nP.o. Da Det Gr3\n1,404,093.1\n45,693,183\n702\n\n\nPolice Department\nSergeant-\n972,381.3\n32,719,211\n486\n\n\nFire Department\nLieutenant\n799,709.9\n27,360,462\n399\n\n\nPolice Department\nLieutenant\n436,343.8\n16,980,478\n218\n\n\nPolice Department\nSchool Safety Agent\n1,398,552.6\n15,999,090\n699\n\n\nFire Department\nCaptain\n386,348.0\n15,092,433\n193\n\n\n\n\n\nThe Commission finds that $850,431,594 can be saved by hiring new employees to reduce overtime expenditure. The above table shows the savings and overtime hours worked for each job and agency. We can see firefighters, sanitation workers, police officers, and correctional officers are all at the top of the list for agencies that can save money by implementing this strategy."
  },
  {
    "objectID": "mp01.html#proposal-3-remove-overtime-pay-and-increase-salaries-by-the-median-or-average-overtime-paid",
    "href": "mp01.html#proposal-3-remove-overtime-pay-and-increase-salaries-by-the-median-or-average-overtime-paid",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "Proposal 3: Remove overtime pay and increase salaries by the median or average overtime paid",
    "text": "Proposal 3: Remove overtime pay and increase salaries by the median or average overtime paid\nOvertime expenditure is a significant portion of the overall payroll for New York City. We will review the financial impact of eliminating overtime all together and replacing it with a one time pay raise. This one time raise will be equal to the median overtime or the average overtime paid per agency and description. Savings are calcualted as the difference of\nThe annualized base pay and the total overtime paid will be used as the base rate instead of the gross rate. This will simplify our analysis, but we lose a degree of accuracy by not including the total other pay for each employee.\nWe will attempt to back-date this analysis by calculating the median overtime paid per agency, job title, and fiscal year. We will then add the median the annualized base pay to create the proposed pay rate. The amount of savings is calculated subtracting the proposed pay rate from the sum of the annualized base pay and the total over paid.\nThe commission will first present the results for 2023 and then compare the results against the annual payroll per year to see the total savings had this methodology been adopted in the past.\n\n\nCode\ntmp &lt;- cleaned |&gt;\n  filter(fiscal_year==2023) |&gt;\n  group_by(fiscal_year, agency_name, title_description) |&gt;\n  summarize(\n    total_base_annual_pay = sum(base_annual_pay),\n    total_over_time_paid = sum(total_ot_paid),\n    median_over_time_paid = median(total_ot_paid),\n    total_ot_hours = sum(ot_hours),\n    number_employees = n(),\n    proposed_total_salary_increase = number_employees * median_over_time_paid\n  ) |&gt;\n  mutate(\n    current_pay_total = total_base_annual_pay + total_over_time_paid,\n    proposed_pay_total = total_base_annual_pay + proposed_total_salary_increase,\n    savings = current_pay_total - proposed_pay_total\n  )\n\n\ntotal_savings_median_pay_raise_2023 &lt;- sum(tmp$savings, na.rm=TRUE)\n\ntmp &lt;- tmp |&gt;\n  group_by(fiscal_year,agency_name) |&gt;\n  summarize(agency_savings = sum(savings, na.rm=TRUE),\n            ) |&gt; \n  ungroup() |&gt;\n  slice_max(agency_savings, n=10) |&gt;\n     rename(\n       \"Fiscal Year\"=fiscal_year,\n       \"Agency Name\"=agency_name,\n        \"Total Savings (USD)\"=agency_savings) \n\ntmp$`Total Savings (USD)` &lt;- format(tmp$`Total Savings (USD)`, big.mark = \",\")\n\nkable(tmp) \n\n\n\n\n\nFiscal Year\nAgency Name\nTotal Savings (USD)\n\n\n\n\n2023\nPolice Department\n101,588,991\n\n\n2023\nHra/Dept Of Social Services\n82,967,212\n\n\n2023\nAdmin For Children’s Svcs\n38,630,245\n\n\n2023\nDepartment Of Correction\n34,806,358\n\n\n2023\nFire Department\n33,940,721\n\n\n2023\nDepartment Of Education Admin\n21,539,934\n\n\n2023\nDepartment Of Sanitation\n21,285,442\n\n\n2023\nDept Of Health/Mental Hygiene\n17,092,481\n\n\n2023\nDept Of Parks & Recreation\n17,035,991\n\n\n2023\nDept Of Environment Protection\n16,796,685\n\n\n\n\n\nIn total for 2023, $463,479,875 is saved using this methodology. The Police Department would save $101,588,991 alone had it used this methodology in 2023. However, the Commission must point out that this may be controversial to propose and that significant push back would be received should this be formally proposed. This option should only be used when the city is facing extreme economic hardships4.\nShould such a need arise, we can see in the below table that the savings are significant each year if we back date this methodology with a minimum saved of $188,159,454 saved in 2015 and a maximum saved of $612,766,377 in 2024.\n\n\nCode\ntmp &lt;- cleaned |&gt;\n  group_by(fiscal_year, agency_name, title_description) |&gt;\n  summarize(\n    average_base_annual_pay = mean(base_annual_pay,na.rm = TRUE ),\n    total_base_annual_pay = sum(base_annual_pay,na.rm = TRUE),\n    total_over_time_paid = sum(total_ot_paid,na.rm = TRUE),\n    median_over_time_paid = median(total_ot_paid,na.rm = TRUE),\n    total_ot_hours = sum(ot_hours),na.rm = TRUE,\n    number_employees = n(),\n    proposed_total_salary_increase = number_employees * median_over_time_paid\n  ) |&gt;\n  mutate(\n    current_pay_total = total_base_annual_pay + total_over_time_paid,\n    proposed_pay_total = total_base_annual_pay + proposed_total_salary_increase,\n    savings = current_pay_total - proposed_pay_total\n  )|&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(savings = sum(savings, na.rm = TRUE)) |&gt;\n  rename(\n    \"Total Annual Payroll Savings (USD)\" = savings,\n    \"Fiscal Year\" = fiscal_year)\n\n  \ntmp$`Total Annual Payroll Savings (USD)` &lt;- format(tmp$`Total Annual Payroll Savings (USD)`, big.mark = \",\") \n\nkable(tmp)\n\n\n\n\nCode\ntime_vec &lt;- c(\"Last Updated:\", format(Sys.time(), \"%Y-%m-%d at %H:%M%p\"))\ncat(paste(time_vec, \" \"))\n\n\nLast Updated:   2025-03-05 at 20:02PM"
  },
  {
    "objectID": "mp01.html#footnotes",
    "href": "mp01.html#footnotes",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.ocb-nyc.org/about/↩︎\nhttps://codelibrary.amlegal.com/codes/newyorkcity/latest/NYCcharter/0-0-0-9↩︎\nhttps://www.newyorker.com/news/news-desk/the-night-new-york-saved-itself-from-bankruptcy↩︎"
  },
  {
    "objectID": "mp01.html#proposal-3-remove-overtime-pay-and-increase-salaries-by-the-median-overtime-paid",
    "href": "mp01.html#proposal-3-remove-overtime-pay-and-increase-salaries-by-the-median-overtime-paid",
    "title": "A Review of the New York City Payroll with Reccomendations for Improvement from the Commission to Analyze Taxpayer Spending (Mini Project 1)",
    "section": "Proposal 3: Remove overtime pay and increase salaries by the median overtime paid",
    "text": "Proposal 3: Remove overtime pay and increase salaries by the median overtime paid\nOvertime expenditure is a significant portion of the overall payroll for New York City. We will review the financial impact of eliminating overtime all together and replacing it with a one time pay raise. This one time raise will be equal to the median overtime paid per agency and job description.\nThe annualized base pay and the total overtime paid will be used as the base rate instead of the gross rate. This will simplify our analysis, but we lose a degree of accuracy by not including the total other pay for each employee.\nWe will attempt to back-date this analysis by calculating the median overtime paid per agency, job title, and fiscal year. We will then take the multiple of the median overtime and the number of employees at the agency with this job title. This product is added to the annualized base pay to create the proposed pay rate in terms of total payroll. The amount of savings is calculated subtracting the proposed payroll total from the sum of the annualized base payroll and the total over paid.\nThe Commission will first present the results for 2023 and then compare the results against the annual payroll per year to see the total savings had this methodology been adopted in the past.\n\n\nCode\nproposal_3_data &lt;- cleaned |&gt;\n  filter(fiscal_year == 2023) |&gt;\n  group_by(fiscal_year, agency_name, title_description) |&gt;\n  summarize(\n    total_base_annual_pay = sum(base_annual_pay),\n    total_over_time_paid = sum(total_ot_paid),\n    median_over_time_paid = median(total_ot_paid),\n    total_ot_hours = sum(ot_hours),\n    number_employees = n(),\n    proposed_total_salary_increase = number_employees * median_over_time_paid\n  ) |&gt;\n  mutate(\n    current_pay_total = total_base_annual_pay + total_over_time_paid,\n    proposed_pay_total = total_base_annual_pay + proposed_total_salary_increase,\n    savings = current_pay_total - proposed_pay_total\n  )\n\n\ntotal_savings_median_pay_raise_2023 &lt;- sum(proposal_3_data$savings, na.rm = TRUE)\n\nproposal_3_data &lt;- proposal_3_data |&gt;\n  group_by(fiscal_year, agency_name) |&gt;\n  summarize(agency_savings = sum(savings, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  slice_max(agency_savings, n = 10) |&gt;\n  rename(\n    \"Fiscal Year\" = fiscal_year,\n    \"Agency Name\" = agency_name,\n    \"Total Savings (USD)\" = agency_savings\n  )\n\nproposal_3_data$`Total Savings (USD)` &lt;- format(proposal_3_data$`Total Savings (USD)`, big.mark = \",\")\n\nkable(proposal_3_data)\n\n\n\n\n\nFiscal Year\nAgency Name\nTotal Savings (USD)\n\n\n\n\n2023\nPolice Department\n101,588,991\n\n\n2023\nHra/Dept Of Social Services\n82,967,212\n\n\n2023\nAdmin For Children’s Svcs\n38,630,245\n\n\n2023\nDepartment Of Correction\n34,806,358\n\n\n2023\nFire Department\n33,940,721\n\n\n2023\nDepartment Of Education Admin\n21,539,934\n\n\n2023\nDepartment Of Sanitation\n21,285,442\n\n\n2023\nDept Of Health/Mental Hygiene\n17,092,481\n\n\n2023\nDept Of Parks & Recreation\n17,035,991\n\n\n2023\nDept Of Environment Protection\n16,796,685\n\n\n\n\n\nIn total for 2023, $463,479,875 is saved using this methodology. The Police Department would save $101,588,991 alone had it used this methodology in 2023. However, the Commission must point out that this may be controversial to propose and that significant push back would be received should this be formally proposed. This option should only be used when the city is facing extreme economic hardships3.\nShould such a need arise, we can see in the below table that the savings are significant each year if we back date this methodology with a minimum saved of $188,159,454 saved in 2015 and a maximum saved of $612,766,377 in 2024.\n\n\nCode\nproposal_3_data_part_2 &lt;- cleaned |&gt;\n  group_by(fiscal_year, agency_name, title_description) |&gt;\n  summarize(\n    total_base_annual_pay = sum(base_annual_pay, na.rm = TRUE),\n    total_over_time_paid = sum(total_ot_paid, na.rm = TRUE),\n    median_over_time_paid = median(total_ot_paid, na.rm = TRUE),\n    total_ot_hours = sum(ot_hours), na.rm = TRUE,\n    number_employees = n(),\n    proposed_total_salary_increase = number_employees * median_over_time_paid\n  ) |&gt;\n  mutate(\n    current_pay_total = total_base_annual_pay + total_over_time_paid,\n    proposed_pay_total = total_base_annual_pay + proposed_total_salary_increase,\n    savings = current_pay_total - proposed_pay_total\n  ) |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(savings = sum(savings, na.rm = TRUE)) |&gt;\n  rename(\n    \"Total Annual Payroll Savings (USD)\" = savings,\n    \"Fiscal Year\" = fiscal_year\n  )\n\n\nproposal_3_data_part_2$`Total Annual Payroll Savings (USD)` &lt;- format(proposal_3_data_part_2$`Total Annual Payroll Savings (USD)`, big.mark = \",\")\n\nkable(proposal_3_data_part_2)\n\n\n\n\n\nFiscal Year\nTotal Annual Payroll Savings (USD)\n\n\n\n\n2014\n216,208,052\n\n\n2015\n201,951,852\n\n\n2016\n304,165,755\n\n\n2017\n356,290,876\n\n\n2018\n370,087,075\n\n\n2019\n360,814,740\n\n\n2020\n340,168,867\n\n\n2021\n370,021,279\n\n\n2022\n483,963,190\n\n\n2023\n496,619,548\n\n\n2024\n654,769,640\n\n\n\n\n\n\n\nLast Updated:   2025-03-05 at 20:54PM"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Craig Allen",
    "section": "",
    "text": "Zicklin School of Business - Baruch College, New York, NY\nM.S. in Statistics, 2024 - Current\nSyracuse University, Syracuse, NY\nB.S. in Mathematics, 2011 - 2015"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Craig Allen",
    "section": "",
    "text": "Zicklin School of Business - Baruch College, New York, NY\nM.S. in Statistics, 2024 - Current\nSyracuse University, Syracuse, NY\nB.S. in Mathematics, 2011 - 2015"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Craig Allen",
    "section": "Experience",
    "text": "Experience\n\nCitigroup\nFD Technologies\nQuantitative Brokers\nDeutsche Bank"
  },
  {
    "objectID": "about.html#trivia",
    "href": "about.html#trivia",
    "title": "Craig Allen",
    "section": "Trivia",
    "text": "Trivia\n\nBooks I’d Reccomend\n\nThe Unaccountability Machine by Dan Davies\n\nThere is No Antimemetics Division by qntm\n\nThis Is How You Lose the Time War by Amal El-Mohtar and Max Gladstone\n\nShogun by James Clavell\n\n\n\nCurrently Reading\nChevengur by Andrey Platonov\n\n\nLikes\n\nCult and B movies\nMeditation\nReading\nTrying to keep bonsai trees alive\nWhen my code works\nCollecting vinyl records\n\n\n\nDislikes\n\nWhen my code doesn’t work\nMoving my vinyl records"
  },
  {
    "objectID": "about.html#song-of-the-week",
    "href": "about.html#song-of-the-week",
    "title": "Craig Allen",
    "section": "Song of the Week",
    "text": "Song of the Week"
  },
  {
    "objectID": "mp02.html#section",
    "href": "mp02.html#section",
    "title": "mini_project_2",
    "section": "",
    "text": "Code\nensure_package &lt;- function(pkg) {\n  pkg &lt;- as.character(substitute(pkg))\n  options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n  if (!require(pkg, character.only = TRUE)) install.packages(pkg)\n  stopifnot(require(pkg, character.only = TRUE))\n}\n\nensure_package(tidyverse)\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\nensure_package(scales)\nensure_package(gt)\nensure_package(ggplot2)\nensure_package(gghighlight)\n\nget_eia_sep &lt;- function(state, abbr) {\n  state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n\n  dir_name &lt;- file.path(\"data\", \"mp02\")\n  file_name &lt;- file.path(dir_name, state_formatted)\n\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n\n  if (!file.exists(file_name)) {\n    BASE_URL &lt;- \"https://www.eia.gov\"\n    REQUEST &lt;- request(BASE_URL) |&gt;\n      req_url_path(\"electricity\", \"state\", state_formatted)\n\n    RESPONSE &lt;- req_perform(REQUEST)\n\n    resp_check_status(RESPONSE)\n\n    writeLines(resp_body_string(RESPONSE), file_name)\n  }\n\n  TABLE &lt;- read_html(file_name) |&gt;\n    html_element(\"table\") |&gt;\n    html_table() |&gt;\n    mutate(Item = str_to_lower(Item))\n\n  if (\"U.S. rank\" %in% colnames(TABLE)) {\n    TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n  }\n\n  CO2_MWh &lt;- TABLE |&gt;\n    filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n    pull(Value) |&gt;\n    str_replace_all(\",\", \"\") |&gt;\n    as.numeric()\n\n  PRIMARY &lt;- TABLE |&gt;\n    filter(Item == \"primary energy source\") |&gt;\n    pull(Rank)\n\n  RATE &lt;- TABLE |&gt;\n    filter(Item == \"average retail price (cents/kwh)\") |&gt;\n    pull(Value) |&gt;\n    as.numeric()\n\n  GENERATION_MWh &lt;- TABLE |&gt;\n    filter(Item == \"net generation (megawatthours)\") |&gt;\n    pull(Value) |&gt;\n    str_replace_all(\",\", \"\") |&gt;\n    as.numeric()\n\n  data.frame(\n    CO2_MWh = CO2_MWh,\n    primary_source = PRIMARY,\n    electricity_price_MWh = RATE * 10, # / 100 cents to dollars &\n    # * 1000 kWh to MWH\n    generation_MWh = GENERATION_MWh,\n    state = state,\n    abbreviation = abbr\n  )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()"
  },
  {
    "objectID": "mp02.html#initial-analysis-of-sep-data",
    "href": "mp02.html#initial-analysis-of-sep-data",
    "title": "mini_project_2",
    "section": "Initial Analysis of SEP Data",
    "text": "Initial Analysis of SEP Data\n\nWhich State has the most expensive retail electricity?\n\n\nCode\nEIA_SEP_REPORT |&gt;\n  slice_max(electricity_price_MWh, n = 1) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Most Expensive Retail Electricity Sources\"\n  ) |&gt;\n  fmt_currency(columns = electricity_price_MWh, decimals = 0) |&gt;\n  fmt_number(columns = c(CO2_MWh, generation_MWh), decimals = 0)\n\n\n\n\nCode\nEIA_SEP_REPORT |&gt;\n  group_by(state) |&gt;\n  summarize(total_co2 = sum(CO2_MWh)) |&gt;\n  slice_max(total_co2, n = 1) |&gt;\n  gt() |&gt;\n  fmt_number(columns = total_co2, decimal = 0)\n\n\n\n\nCode\nweighted.mean(EIA_SEP_REPORT$CO2_MWh, w = (EIA_SEP_REPORT$generation_MWh / sum(EIA_SEP_REPORT$generation_MWh)))\n\n\n\n\nCode\nEIA_SEP_REPORT |&gt;\n  group_by(primary_source) |&gt;\n  summarize(count = n()) |&gt;\n  arrange(count) |&gt;\n  gt()\n\n\n\n\nCode\nEIA_SEP_REPORT |&gt;\n  filter(state %in% c(\"New York\", \"Texas\")) |&gt;\n  group_by(state) |&gt;\n  summarize(total_co2 = sum(CO2_MWh)) |&gt;\n  gt()\n\n\n\n\nCode\nensure_package(readxl)\n# Create 'data/mp02' directory if not already present\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings = FALSE, recursive = TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\nNTD_ENERGY_FUEL_TYPE_EMISSIONS_FILE &lt;- file.path(DATA_DIR, \"c02_vol_mass.xlsx\")\n\n\n\nif (!file.exists(NTD_ENERGY_FILE)) {\n  DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\",\n    destfile = NTD_ENERGY_FILE,\n    method = \"curl\"\n  )\n\n  if (DS | (file.info(NTD_ENERGY_FILE)$size == 0)) {\n    cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nif (!file.exists(NTD_ENERGY_FUEL_TYPE_EMISSIONS_FILE)) {\n  DS &lt;- download.file(\"https://www.eia.gov/environment/emissions/xls/co2_vol_mass.xlsx\",\n    destfile = NTD_ENERGY_FUEL_TYPE_EMISSIONS_FILE,\n    method = \"curl\"\n  )\n\n  if (DS | (file.info(NTD_ENERGY_FUEL_TYPE_EMISSIONS_FILE)$size == 0)) {\n    cat(\"I was unable to download the NTD Fuel Type Emissions File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\n\n\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE, na = c(\"\", \"-\"))\nNTD_FUEL_TYPE_EMISSIONS &lt;- read_xlsx(NTD_ENERGY_FUEL_TYPE_EMISSIONS_FILE)\n# NTD_FUEL_TYPE_EMISSIONS &lt;- NTD_FUEL_TYPE_EMISSIONS[-c(1,3,12,15,17,24,30,36,37,38,39,40,41),][-c(3,5)]\n\n\n\n\nCode\nensure_package(tidyr)\nto_numeric_fill_0 &lt;- function(x) {\n  replace_na(as.numeric(x), 0)\n}\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt;\n  select(-c(\n    `Reporter Type`,\n    `Reporting Module`,\n    `Other Fuel`,\n    `Other Fuel Description`\n  )) |&gt;\n  mutate(across(\n    -c(\n      `Agency Name`,\n      `Mode`,\n      `TOS`\n    ),\n    to_numeric_fill_0\n  )) |&gt;\n  group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n  summarize(across(where(is.numeric), sum),\n    .groups = \"keep\"\n  ) |&gt;\n  mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n  filter(ENERGY &gt; 0) |&gt;\n  select(-ENERGY) |&gt;\n  ungroup()\n\nrm(NTD_ENERGY_RAW)\n\n\n\n\nCode\n## Mode Codes sourced from NTD Glossary https://www.transit.dot.gov/ntd/national-transit-database-ntd-glossary#M\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"DR\" ~ \"Double Decker Buses\",\n    Mode == \"FB\" ~ \"Ferry Boat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MG\" ~ \"Monorail/Automated guideway transit\",\n    Mode == \"SR\" ~ \"Streetcar\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"VP\" ~ \"Van Pool\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\n\n\nCode\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif (!file.exists(NTD_SERVICE_FILE)) {\n  DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\",\n    destfile = NTD_SERVICE_FILE,\n    method = \"curl\"\n  )\n\n  if (DS | (file.info(NTD_SERVICE_FILE)$size == 0)) {\n    cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n  mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt;\n  rename(\n    Agency = agency,\n    City = max_city,\n    State = max_state,\n    UPT = sum_unlinked_passenger_trips_upt,\n    MILES = sum_passenger_miles\n  ) |&gt;\n  select(matches(\"^[A-Z]\", ignore.case = FALSE)) |&gt;\n  filter(MILES &gt; 0)\n\nrm(NTD_SERVICE_RAW)\n\n\n\n\nCode\nNTD_SERVICE |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_UPT = sum(UPT)) |&gt;\n  slice_max(total_UPT, n = 1) |&gt;\n  gt() |&gt;\n  fmt_number(columns = total_UPT, decimal = 0)\n\n\n\n\nCode\nNTD_SERVICE |&gt;\n  filter(\n    State == \"NY\",\n    City %in% c(\"Staten Island\", \"New York\", \"Brooklyn\")\n  ) |&gt;\n  # miles per trip\n  # TODO filter out non MTA\n  mutate(trip_length = MILES / UPT) |&gt;\n  summarize(avg_trip_length = mean(trip_length))\n\n\n\n\nCode\nNTD_SERVICE |&gt;\n  filter(\n    State == \"NY\",\n    City %in% c(\"Staten Island\", \"New York\", \"Brooklyn\")\n  ) |&gt;\n  mutate(trip_length = MILES / UPT) |&gt;\n  # miles per trip\n  slice_max(trip_length, n = 1) |&gt;\n  gt()\n\n\n\n\nCode\nNTD_SERVICE |&gt;\n  group_by(State) |&gt;\n  summarize(tot_miles = sum(MILES)) |&gt;\n  slice_min(tot_miles, n = 1) |&gt;\n  gt()\n\n\n\n\nCode\nNTD_SERVICE |&gt;\n  select(State) |&gt;\n  unique() |&gt;\n  lengths()\n# TODO Also Check DC and Puerto Rico\nsetdiff(data.frame(State = state.abb), data.frame(State = NTD_SERVICE$State))\n# TODO write out the\n\n\n19 States are not included in the NTD Service Data: Arizona, Arkansas, California, Colorado, Hawaii, iowa, Kansas, Louisianna, Missouri, Montana , Nebraska. Nevada, New Mexico, North Dakota, Oklahoma, South Dakota, Utah, Texas, and Wyoming.\nWe must filter out Puerto Rico and DC because of the lack of energy costs as well.\n\n\nCode\nstate_key &lt;- data.frame(state_abb = state.abb, state_name = state.name)\n# Additional source https://www.engineeringtoolbox.com/co2-emission-fuels-d_1085.html\nconsolidated &lt;- NTD_SERVICE |&gt;\n  inner_join(NTD_ENERGY, by = join_by(`NTD ID` == `NTD ID`)) |&gt;\n  left_join(EIA_SEP_REPORT, by = join_by(State == abbreviation)) |&gt;\n  mutate(\n    total_emissions = `Bio-Diesel` * 20.9 + `Bunker Fuel` * 25.5 + `C Natural Gas` * 120.85 + `Diesel Fuel` * 22.45 + `Electric Battery` * CO2_MWh / 1000 + `Electric Propulsion` * CO2_MWh / 1000 + `Kerosene` * 20.6 + Ethanol * 12.6 + Methonal * 9.1 + Gasoline * 20.86 + Hydrogen * CO2_MWh / 1000 + `Liquified Nat Gas` * 120.85 + `Liquified Petroleum Gas` * 12.68,\n    clean_energy = case_when(\n      primary_source %in% c(\"Hydroelectric\", \"Nuclear\", \"Wind\") ~ `Electric Battery` * CO2_MWh / 1000 + `Electric Propulsion` * CO2_MWh / 1000 + Hydrogen * CO2_MWh / 1000 + Hydrogen * CO2_MWh / 1000 ,\n      TRUE ~ 0\n    )\n  ) |&gt;\n  filter(!(State %in% c(\"PR\", \"DC\"))) |&gt;\n  unique()\n\n\n\n\nCode\nresults &lt;- consolidated |&gt;\n  mutate(size = case_when(\n    UPT &gt; quantile(UPT, .75) ~ \"Large\",\n    UPT &gt; quantile(UPT, .25) ~ \"Medium\",\n    UPT &lt;= quantile(UPT, .25) ~ \"Small\"\n  )) |&gt;\n  group_by(Agency, UPT, MILES, State, size) |&gt;\n  summarize(\n    tot_em = sum(total_emissions, na.rm = TRUE),\n    tot_em_per_upt = tot_em / UPT,\n    tot_em_per_mile = tot_em / MILES,\n    total_clean_energy = sum(clean_energy, na.rm=TRUE),\n  ) |&gt;\n  ungroup() |&gt;\n  unique()\n\nresults$size &lt;- factor(results$size, unique(results$size))\n\n\n\n\nCode\nresults |&gt;\n  filter(size == \"Large\") |&gt;\n  slice_min(tot_em_per_upt, n = 1)\n\nresults |&gt;\n  filter(size == \"Medium\") |&gt;\n  slice_min(tot_em_per_upt, n = 1)\n\nresults |&gt;\n  filter(size == \"Small\") |&gt;\n  slice_min(tot_em_per_upt, n = 1)\n\n\n\n\nCode\nresults |&gt;\n  filter(size == \"Large\") |&gt;\n  arrange(tot_em_per_upt)|&gt;\n  mutate(Agency=factor(Agency,levels=Agency))|&gt;\n  ggplot(aes(x=Agency,y=tot_em_per_upt)) +\n  geom_bar(stat = \"identity\") +\n    scale_y_continuous(\n    name = \"Total Emisssions Per UPT\",\n    breaks = scales::pretty_breaks(n = 6)\n  ) +\n  scale_x_discrete(breaks = function(x){x[c(TRUE, FALSE)]},labels=abbreviate, name = \"Agency Name (Abbreviated)\") +\n  theme(axis.text.x=element_text(angle = -45, hjust = 0)) + \n  gghighlight(tot_em_per_upt &lt;0.473) \n\n\n\n\nCode\n# CAFE Rate is 49 MPG\nresults |&gt;\n  mutate(\n    car_equivalent_em = 49 * MILES * 20.86,\n    avoided_emissions = car_equivalent_em - tot_em\n  ) |&gt;\n  slice_max(avoided_emissions, n = 10) |&gt;\n  select(Agency, avoided_emissions)\n\n\n\n\nCode\nresults |&gt;\n  slice_max(total_clean_energy,n=1)\n\n\n\n\nCode\nresults |&gt;\n  filter(total_clean_energy&gt;0) |&gt;\n  arrange(desc(total_clean_energy)) |&gt;\n  mutate(Agency=factor(Agency,levels=Agency))|&gt;\n  ggplot(aes(x=Agency,y=total_clean_energy)) +\n  geom_bar(stat = \"identity\") +\n    scale_y_continuous(\n    name = \"Total Amount of Clean Energy Used\",\n    breaks = scales::pretty_breaks(n = 6)\n  ) +\n  scale_x_discrete(breaks = function(x){x[c(TRUE, FALSE)]},labels=abbreviate, name = \"Agency Name (Abbreviated)\") +\n  theme(axis.text.x=element_text(angle = -45, hjust = 0)) \n\n\n\n\nCode\nresults |&gt;\n  filter(size == \"Small\") |&gt;\n  slice_max(tot_em_per_upt, n = 1)\n\n\n[^1] GTA Vice City"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Green Transit Alliance for Investigation of Variance Annual Awards (Mini Project 2)",
    "section": "",
    "text": "The Green Transit Alliance for Investigation of Variance1 is pleased to announce the winners of the coveted Victory over Internal Combustion Engines in the City (Vice City) awards. The awards were presented during a live ceremony at the San Fierro City Hall in San Andreas, California on March 22nd.\nWithout further ado, GTA IV is pleased to announce the illustrious winners of the 2025 Vice City Awards.\n\n\n🏆 Greenest Transportation Agency - Large : TriMet of Oregon\n🏆 Greenest Transportation Agency - Medium : Seattle Center Monorail\n🏆 Greenest Transportation Agency - Small : City of Fort Lauderdale\n🏆 Most Emissions Avoided : MTA New York City Transit\n🏆 Highest Electric Vehicle Usage with Clean Power Sources : Chicago Transit Authority\n🚫🏆🚫 Better Luck Next Time - Most Emissions Per Mile for a Small Agency : Altoona Metro Transit"
  },
  {
    "objectID": "mp02.html#winners",
    "href": "mp02.html#winners",
    "title": "Green Transit Alliance for Investigation of Variance Annual Awards (Mini Project 2)",
    "section": "",
    "text": "🏆 Greenest Transportation Agency - Large : TriMet of Oregon\n🏆 Greenest Transportation Agency - Medium : Seattle Center Monorail\n🏆 Greenest Transportation Agency - Small : City of Fort Lauderdale\n🏆 Most Emissions Avoided : MTA New York City Transit\n🏆 Highest Electric Vehicle Usage with Clean Power Sources : Chicago Transit Authority\n🚫🏆🚫 Better Luck Next Time - Most Emissions Per Mile for a Small Agency : Altoona Metro Transit"
  },
  {
    "objectID": "mp02.html#exploratory-efforts",
    "href": "mp02.html#exploratory-efforts",
    "title": "Green Transit Alliance for Investigation of Variance Annual Awards (Mini Project 2)",
    "section": "Exploratory Efforts",
    "text": "Exploratory Efforts\n\n\nCode\nensure_package &lt;- function(pkg) {\n  pkg &lt;- as.character(substitute(pkg))\n  options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n  if (!require(pkg, character.only = TRUE)) install.packages(pkg)\n  stopifnot(require(pkg, character.only = TRUE))\n}\n\nensure_package(tidyverse)\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\nensure_package(scales)\nensure_package(gt)\nensure_package(ggplot2)\nensure_package(gghighlight)\n\nget_eia_sep &lt;- function(state, abbr) {\n  state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n\n  dir_name &lt;- file.path(\"data\", \"mp02\")\n  file_name &lt;- file.path(dir_name, state_formatted)\n\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n\n  if (!file.exists(file_name)) {\n    BASE_URL &lt;- \"https://www.eia.gov\"\n    REQUEST &lt;- request(BASE_URL) |&gt;\n      req_url_path(\"electricity\", \"state\", state_formatted)\n\n    RESPONSE &lt;- req_perform(REQUEST)\n\n    resp_check_status(RESPONSE)\n\n    writeLines(resp_body_string(RESPONSE), file_name)\n  }\n\n  TABLE &lt;- read_html(file_name) |&gt;\n    html_element(\"table\") |&gt;\n    html_table() |&gt;\n    mutate(Item = str_to_lower(Item))\n\n  if (\"U.S. rank\" %in% colnames(TABLE)) {\n    TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n  }\n\n  CO2_MWh &lt;- TABLE |&gt;\n    filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n    pull(Value) |&gt;\n    str_replace_all(\",\", \"\") |&gt;\n    as.numeric()\n\n  PRIMARY &lt;- TABLE |&gt;\n    filter(Item == \"primary energy source\") |&gt;\n    pull(Rank)\n\n  RATE &lt;- TABLE |&gt;\n    filter(Item == \"average retail price (cents/kwh)\") |&gt;\n    pull(Value) |&gt;\n    as.numeric()\n\n  GENERATION_MWh &lt;- TABLE |&gt;\n    filter(Item == \"net generation (megawatthours)\") |&gt;\n    pull(Value) |&gt;\n    str_replace_all(\",\", \"\") |&gt;\n    as.numeric()\n\n  data.frame(\n    CO2_MWh = CO2_MWh,\n    primary_source = PRIMARY,\n    electricity_price_MWh = RATE * 10, # / 100 cents to dollars &\n    # * 1000 kWh to MWH\n    generation_MWh = GENERATION_MWh,\n    state = state,\n    abbreviation = abbr\n  )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\n\n\nWhich State has the most expensive retail electricity?\n\n\nCode\nEIA_SEP_REPORT |&gt;\n  slice_max(electricity_price_MWh, n = 1) |&gt;\n  rename(\n    `Pounds of CO2 Emitted per MWh of Electricity Produced` = CO2_MWh,\n    `Primary Source of Electricity Generation` = primary_source,\n    `Average Retail Price for 1000 kWh` = electricity_price_MWh,\n    `Total Generation Capacity (MWh)` = generation_MWh,\n    State = state\n  ) |&gt;\n  select(-c(abbreviation)) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Most Expensive Retail Electricity Sources\"\n  ) |&gt;\n  fmt_currency(columns = `Average Retail Price for 1000 kWh`, decimals = 0) |&gt;\n  fmt_number(columns = c(`Pounds of CO2 Emitted per MWh of Electricity Produced`, `Total Generation Capacity (MWh)`), decimals = 0)\n\n\n\n\n\n\n\n\nMost Expensive Retail Electricity Sources\n\n\nPounds of CO2 Emitted per MWh of Electricity Produced\nPrimary Source of Electricity Generation\nAverage Retail Price for 1000 kWh\nTotal Generation Capacity (MWh)\nState\n\n\n\n\n1,444\nPetroleum\n$386\n9,194,164\nHawaii\n\n\n\n\n\n\n\nHawaii has the most expensive retail energy source at $386 per 1000 kWh.\n\n\nWhich State has the dirtiest energy mix?\n\n\nCode\nEIA_SEP_REPORT |&gt;\n  filter(primary_source %in% c(\"Natural gas\", \"Coal\", \"Petrolum\")) |&gt;\n  group_by(state) |&gt;\n  summarize(total_co2 = sum(CO2_MWh * generation_MWh)) |&gt;\n  slice_max(total_co2, n = 1) |&gt;\n  rename(State = state, `Total Pounds of CO2 Emitted from Electricity Produced` = total_co2) |&gt;\n  gt() |&gt;\n  fmt_number(columns = `Total Pounds of CO2 Emitted from Electricity Produced`, decimal = 0) |&gt;\n  tab_header(\n    title = \"The State with the Dirtiest Mix of Energy\"\n  )\n\n\n\n\n\n\n\n\nThe State with the Dirtiest Mix of Energy\n\n\nState\nTotal Pounds of CO2 Emitted from Electricity Produced\n\n\n\n\nTexas\n467,936,841,960\n\n\n\n\n\n\n\nTexas is the dirtiest state with 467,936,841,960 pounds of CO2 produced. This is assuming that all of the electricity producted in Texas is done so with it’s primary source of Natural gas.\n\n\nOn average, how many pounds of CO2 are emitted per MWh of electricity produced in the US?\n\n\nCode\navg &lt;- weighted.mean(EIA_SEP_REPORT$CO2_MWh, w = (EIA_SEP_REPORT$generation_MWh / sum(EIA_SEP_REPORT$generation_MWh)))\n\n\nAbout 805 pounds of CO2 is produced per MWh of energy produced accoring to the weight average of CO2 produced per MWH and the generation capacity of each state.\n\n\nWhat is the rarest primary source of energy in the country?\n\n\nCode\nEIA_SEP_REPORT |&gt;\n  group_by(primary_source) |&gt;\n  summarize(Count = n()) |&gt;\n  arrange(Count) |&gt;\n  rename(`Primary Source` = primary_source) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Count of Primary Energy Sources for each State\"\n  )\n\n\n\n\n\n\n\n\nCount of Primary Energy Sources for each State\n\n\nPrimary Source\nCount\n\n\n\n\nPetroleum\n1\n\n\nHydroelectric\n4\n\n\nNuclear\n4\n\n\nWind\n5\n\n\nCoal\n10\n\n\nNatural gas\n26\n\n\n\n\n\n\n\nPetroleum is the least common primary energy source in the United States with only a single state using it as such.\n\n\nTexas vs. New York\n\n\nCode\nEIA_SEP_REPORT |&gt;\n  filter(state %in% c(\"New York\", \"Texas\")) |&gt;\n  group_by(state) |&gt;\n  summarize(total_co2 = sum(CO2_MWh * generation_MWh)) |&gt;\n  rename(State = state, `Total CO2 Emiited` = total_co2) |&gt;\n  gt() |&gt;\n  fmt_number(`Total CO2 Emiited`, decimal = 0)\n\n\n\n\n\n\n\n\nState\nTotal CO2 Emiited\n\n\n\n\nNew York\n64,748,873,736\n\n\nTexas\n467,936,841,960\n\n\n\n\n\n\n\nWe can see a striking difference in the amount of CO2 emitted between New York and Texas. Texas emitted approximately 7 times the amount of CO2 as New York does.\n\n\nCode\nensure_package(readxl)\n# Create 'data/mp02' directory if not already present\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings = FALSE, recursive = TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\nNTD_ENERGY_FUEL_TYPE_EMISSIONS_FILE &lt;- file.path(DATA_DIR, \"c02_vol_mass.xlsx\")\n\n\n\nif (!file.exists(NTD_ENERGY_FILE)) {\n  DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\",\n    destfile = NTD_ENERGY_FILE,\n    method = \"curl\"\n  )\n\n  if (DS | (file.info(NTD_ENERGY_FILE)$size == 0)) {\n    cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nif (!file.exists(NTD_ENERGY_FUEL_TYPE_EMISSIONS_FILE)) {\n  DS &lt;- download.file(\"https://www.eia.gov/environment/emissions/xls/co2_vol_mass.xlsx\",\n    destfile = NTD_ENERGY_FUEL_TYPE_EMISSIONS_FILE,\n    method = \"curl\"\n  )\n\n  if (DS | (file.info(NTD_ENERGY_FUEL_TYPE_EMISSIONS_FILE)$size == 0)) {\n    cat(\"I was unable to download the NTD Fuel Type Emissions File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\n\n\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE, na = c(\"\", \"-\"))\nNTD_FUEL_TYPE_EMISSIONS &lt;- read_xlsx(NTD_ENERGY_FUEL_TYPE_EMISSIONS_FILE)\n# NTD_FUEL_TYPE_EMISSIONS &lt;- NTD_FUEL_TYPE_EMISSIONS[-c(1,3,12,15,17,24,30,36,37,38,39,40,41),][-c(3,5)]\n\n\n\n\nCode\nensure_package(tidyr)\nto_numeric_fill_0 &lt;- function(x) {\n  replace_na(as.numeric(x), 0)\n}\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt;\n  select(-c(\n    `Reporter Type`,\n    `Reporting Module`,\n    `Other Fuel`,\n    `Other Fuel Description`\n  )) |&gt;\n  mutate(across(\n    -c(\n      `Agency Name`,\n      `Mode`,\n      `TOS`\n    ),\n    to_numeric_fill_0\n  )) |&gt;\n  group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n  summarize(across(where(is.numeric), sum),\n    .groups = \"keep\"\n  ) |&gt;\n  mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n  filter(ENERGY &gt; 0) |&gt;\n  select(-ENERGY) |&gt;\n  ungroup()\n\nrm(NTD_ENERGY_RAW)\n\n\n\n\nCode\n## Mode Codes sourced from NTD Glossary https://www.transit.dot.gov/ntd/national-transit-database-ntd-glossary#M\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"DR\" ~ \"Double Decker Buses\",\n    Mode == \"FB\" ~ \"Ferry Boat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MG\" ~ \"Monorail/Automated guideway transit\",\n    Mode == \"SR\" ~ \"Streetcar\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"VP\" ~ \"Van Pool\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\n\n\nCode\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif (!file.exists(NTD_SERVICE_FILE)) {\n  DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\",\n    destfile = NTD_SERVICE_FILE,\n    method = \"curl\"\n  )\n\n  if (DS | (file.info(NTD_SERVICE_FILE)$size == 0)) {\n    cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n  mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt;\n  rename(\n    Agency = agency,\n    City = max_city,\n    State = max_state,\n    UPT = sum_unlinked_passenger_trips_upt,\n    MILES = sum_passenger_miles\n  ) |&gt;\n  select(matches(\"^[A-Z]\", ignore.case = FALSE)) |&gt;\n  filter(MILES &gt; 0)\n\nrm(NTD_SERVICE_RAW)\n\n\n\n\nWhich transit service has the most UPT annually?\n\n\nCode\nNTD_SERVICE |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_UPT = sum(UPT)) |&gt;\n  slice_max(total_UPT, n = 1) |&gt;\n  rename(`Total Unlinked Passenger Trips`=total_UPT) |&gt;\n  gt() |&gt;\n  fmt_number(columns = `Total Unlinked Passenger Trips`, decimal = 0) |&gt;\n  tab_header(title='Transit Agency with the Highest Annual Number of Total Unlinked Passenger Trips')\n\n\n\n\n\n\n\n\nTransit Agency with the Highest Annual Number of Total Unlinked Passenger Trips\n\n\nAgency\nTotal Unlinked Passenger Trips\n\n\n\n\nMTA New York City Transit\n2,632,003,044\n\n\n\n\n\n\n\nThe MTA of New York City has the largest annual amount of Total Unlinked Passenger trips at well over 2.6 billion trips.\n\n\nHow long is the average trip length in New York City?\n\n\nCode\nNTD_SERVICE |&gt;\n  filter(\n    State == \"NY\",\n    City %in% c(\"Staten Island\", \"New York\", \"Brooklyn\")\n  ) |&gt;\n  # miles per trip\n  # TODO filter out non MTA\n  mutate(trip_length = MILES / UPT) |&gt;\n  summarize(avg_trip_length = mean(trip_length)) |&gt;\n  rename(`Average Trip Length`=avg_trip_length) |&gt;\n  gt()\n\n\nWe can see that the average trip length in New York City is about 8.25 miles.\n\n\nCode\nNTD_SERVICE |&gt;\n  filter(\n    State == \"NY\",\n    City %in% c(\"Staten Island\", \"New York\", \"Brooklyn\")\n  ) |&gt;\n  mutate(trip_length = MILES / UPT) |&gt;\n  # miles per trip\n  slice_max(trip_length, n = 1) |&gt;\n  select(-c(City,State,MILES,UPT,`NTD ID`)) |&gt;\n  rename(`Trip Length`=trip_length) |&gt;\n  gt() |&gt;\n  tab_header(title = \"Agency with Longest Average Trip Length\")\n\n\n\n\n\n\n\n\nAgency with Longest Average Trip Length\n\n\nAgency\nTrip Length\n\n\n\n\nMTA Long Island Rail Road\n24.25799\n\n\n\n\n\n\n\nThe service with the longest average trip length among the New York City agencies is the Long Island Rail Road.\n\n\nWhat state has the total fewest miles travelled by Public Transit?\n\n\nCode\nNTD_SERVICE |&gt;\n  group_by(State) |&gt;\n  summarize(tot_miles = sum(MILES)) |&gt;\n  slice_min(tot_miles, n = 1) |&gt;\n  rename(`Total Miles Travveled by Public Transit`=tot_miles) |&gt;\n  gt() |&gt;\n  fmt_number(`Total Miles Travveled by Public Transit`,decimal=0)\n\n\n\n\n\n\n\n\nState\nTotal Miles Travveled by Public Transit\n\n\n\n\nNH\n3,749,892\n\n\n\n\n\n\n\nNew Hampshite has the fewest total number of miles travelled at ~3.75 million miles.\n\n\nAre all the states present in the Service data?\n\n\nCode\nsetdiff(data.frame(State = state.abb), data.frame(State = NTD_SERVICE$State)) |&gt;\n  gt() |&gt;\n  tab_header(title=\"States Not Included in the Transit Dataset\")\n\n\n\n\n\n\n\n\nStates Not Included in the Transit Dataset\n\n\nState\n\n\n\n\nAZ\n\n\nAR\n\n\nCA\n\n\nCO\n\n\nHI\n\n\nIA\n\n\nKS\n\n\nLA\n\n\nMO\n\n\nMT\n\n\nNE\n\n\nNV\n\n\nNM\n\n\nND\n\n\nOK\n\n\nSD\n\n\nTX\n\n\nUT\n\n\nWY\n\n\n\n\n\n\n\n19 States are not included in the NTD Service Data: Arizona, Arkansas, California, Colorado, Hawaii, Iowa, Kansas, Louisianna, Missouri, Montana , Nebraska. Nevada, New Mexico, North Dakota, Oklahoma, South Dakota, Utah, Texas, and Wyoming.\nWe also note that Washington D.C. and Puerto Rico are included in the service data."
  },
  {
    "objectID": "mp02.html#footnotes",
    "href": "mp02.html#footnotes",
    "title": "Green Transit Alliance for Investigation of Variance Annual Awards (Mini Project 2)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot to be confused with the similarly named Green Transit Alliance for Variance or the Green Transit Alliance for Variance of Investigations.↩︎"
  },
  {
    "objectID": "mp02.html#awards-analysis",
    "href": "mp02.html#awards-analysis",
    "title": "Green Transit Alliance for Investigation of Variance Annual Awards (Mini Project 2)",
    "section": "Awards Analysis",
    "text": "Awards Analysis\n\n\nCode\nstate_key &lt;- data.frame(state_abb = state.abb, state_name = state.name)\n# Additional source https://www.engineeringtoolbox.com/co2-emission-fuels-d_1085.html\nconsolidated &lt;- NTD_SERVICE |&gt;\n  inner_join(NTD_ENERGY, by = join_by(`NTD ID` == `NTD ID`)) |&gt;\n  left_join(EIA_SEP_REPORT, by = join_by(State == abbreviation)) |&gt;\n  mutate(\n    total_emissions = `Bio-Diesel` * 20.9 + `Bunker Fuel` * 25.5 + `C Natural Gas` * 120.85 + `Diesel Fuel` * 22.45 + `Electric Battery` * CO2_MWh / 1000 + `Electric Propulsion` * CO2_MWh / 1000 + `Kerosene` * 20.6 + Ethanol * 12.6 + Methonal * 9.1 + Gasoline * 20.86 + Hydrogen * CO2_MWh / 1000 + `Liquified Nat Gas` * 120.85 + `Liquified Petroleum Gas` * 12.68,\n    clean_energy = case_when(\n      primary_source %in% c(\"Hydroelectric\", \"Nuclear\", \"Wind\") ~ `Electric Battery`  / 1000 + `Electric Propulsion` / 1000 ,\n      TRUE ~ 0\n    )\n  ) |&gt;\n  filter(!(State %in% c(\"PR\", \"DC\"))) |&gt;\n  unique()\n\n\n\n\nCode\nresults &lt;- consolidated |&gt;\n  mutate(size = case_when(\n    UPT &gt; quantile(UPT, .75) ~ \"Large\",\n    UPT &gt; quantile(UPT, .25) ~ \"Medium\",\n    UPT &lt;= quantile(UPT, .25) ~ \"Small\"\n  )) |&gt;\n  group_by(Agency, UPT, MILES, State, size) |&gt;\n  summarize(\n    tot_em = sum(total_emissions, na.rm = TRUE),\n    tot_em_per_upt = tot_em / UPT,\n    tot_em_per_mile = tot_em / MILES,\n    total_clean_energy = sum(clean_energy, na.rm = TRUE),\n  ) |&gt;\n  ungroup() |&gt;\n  unique()\n\nresults$size &lt;- factor(results$size, unique(results$size))\n\n\n\n🏆 Greenest Transportation Agency\nThe Green Transit Alliance for Investigation of Variance looked at the total amount of emissions per Unlinked Passenger Trip as the main metric for detmining the Greenest Transit Agency. We award this to three agencies based on the total number of Unlinked Passenger Trips serviced by the agencies.\n\nLarge Agency\n\n\nCode\nresults |&gt;\n  filter(size == \"Large\") |&gt;\n  slice_min(tot_em_per_upt, n = 1) |&gt;\n  select(Agency,tot_em_per_upt) |&gt;\n  rename(`Total Emissions Per UPT`=tot_em_per_upt)|&gt;\n  gt()\n\n\n\n\n\n\n\n\nAgency\nTotal Emissions Per UPT\n\n\n\n\nTri-County Metropolitan Transportation District of Oregon, dba: TriMet\n0.4727423\n\n\n\n\n\n\n\n\n\nCode\nresults |&gt;\n  filter(size == \"Large\") |&gt;\n  arrange(tot_em_per_upt) |&gt;\n  mutate(Agency = factor(Agency, levels = Agency)) |&gt;\n  ggplot(aes(x = Agency, y = tot_em_per_upt)) +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(\n    name = \"Total Pounds of CO2 Emisssions Per UPT\",\n    breaks = scales::pretty_breaks(n = 6)\n  ) +\n  scale_x_discrete(breaks = function(x) {\n    x[c(TRUE, FALSE)]\n  }, labels = abbreviate, name = \"Agency Name (Abbreviated)\") +\n  theme(axis.text.x = element_text(angle = -45, hjust = 0)) +\n  gghighlight(tot_em_per_upt &lt; 0.473) + \n  labs(title=\"Emissions per UPT for Agencies in Scope\")\n\n\n\n\n\n\n\n\n\nWe can see on the chart that TriMet of Oregon (highlighted) has significantly fewer emissions per UPT than the other large agencies. Names have been removed or abbreviated to save space.\n\n\nMedium Agency\n\n\nCode\nresults |&gt;\n  filter(size == \"Medium\") |&gt;\n  slice_min(tot_em_per_upt, n = 1) |&gt;\n  select(Agency,tot_em_per_upt)|&gt;\n  rename(`Total Emissions Per UPT`=tot_em_per_upt)|&gt;\n  gt()\n\n\n\n\n\n\n\n\nAgency\nTotal Emissions Per UPT\n\n\n\n\nCity of Seattle, dba: Seattle Center Monorail\n0.06915565\n\n\n\n\n\n\n\nThe Seattle Monorail is the greenest in the Medium size-class of agencies.\n\n\nSmall Agency\n\n\nCode\nresults |&gt;\n  filter(size == \"Small\") |&gt;\n  slice_min(tot_em_per_upt, n = 1)|&gt;\n  select(Agency,tot_em_per_upt)|&gt;\n  rename(`Total Emissions Per UPT`=tot_em_per_upt)|&gt;\n  gt()\n\n\n\n\n\n\n\n\nAgency\nTotal Emissions Per UPT\n\n\n\n\nCity of Fort Lauderdale\n2.507277\n\n\n\n\n\n\n\nThe City of Fort Lauderdale operates the Greenest Agency in the Small Category.\n\n\n\n🏆 Most Emmissions Avoided\n\n\nCode\n# CAFE Rate is 49 MPG\nresults |&gt;\n  mutate(\n    car_equivalent_em = 49 * MILES * 20.86,\n    avoided_emissions = car_equivalent_em - tot_em\n  ) |&gt;\n  slice_max(avoided_emissions, n = 10) |&gt;\n  select(Agency, avoided_emissions)|&gt; \n  rename(`Total Avoided Pounds of Emissions`=avoided_emissions)|&gt;\n  gt()|&gt; \n  fmt_number(`Total Avoided Pounds of Emissions`,decimal=0)\n\n\n\n\n\n\n\n\nAgency\nTotal Avoided Pounds of Emissions\n\n\n\n\nMTA New York City Transit\n9,801,471,738,861\n\n\nNew Jersey Transit Corporation\n2,364,399,826,776\n\n\nMTA Long Island Rail Road\n2,078,266,426,029\n\n\nMetro-North Commuter Railroad Company, dba: MTA Metro-North Railroad\n1,176,001,745,021\n\n\nMassachusetts Bay Transportation Authority\n1,127,032,929,054\n\n\nChicago Transit Authority\n1,114,342,911,591\n\n\nSoutheastern Pennsylvania Transportation Authority\n852,829,315,305\n\n\nCounty of Miami-Dade\n422,398,792,227\n\n\nMetropolitan Atlanta Rapid Transit Authority\n359,197,182,034\n\n\nMTA Bus Company\n355,221,268,365\n\n\n\n\n\n\n\nThe MTA of New York City avoided the most emissions: over 9 billion pounds of CO2. Avoided emissions were calculated by taking the number of miles travelled on the service and using the equivalent emissions if those miles were driven by a gasoline powered car.\n\n\n🏆 Highest Electric Vehicle Usage with Clean Power Sources\n\n\nCode\nresults |&gt;\n  slice_max(total_clean_energy, n = 1) |&gt; \n  select(Agency,total_clean_energy) |&gt;\n  rename(`kWh of Clean Energy Used for Electric Vehicles`=total_clean_energy) |&gt;\n  gt() |&gt;\n  fmt_number(`kWh of Clean Energy Used for Electric Vehicles`,decimal=0) |&gt; \n  tab_header(title=\"Agency with the Most Clean Energy Used to Power Electric Vehicles\")\n\n\nFor this award, we look at the transit agencies with respect to the primary power source in their state. If the primary power source is nuclear, wind, or hydroelectric, we take the amount of kWh used for electric vehicles. The agency that has the highest amount of electric vehicle usage with a clean energy source is the Chicago Transit Authority with 340 Wh of clean energy used.\nWe can see in the graph below just how much the Chicago Transit Authority (highlighted) beats out its competitors.\n\n\nCode\nresults |&gt;\n  filter(total_clean_energy &gt; 0) |&gt;\n  arrange(desc(total_clean_energy)) |&gt;\n  mutate(Agency = factor(Agency, levels = Agency)) |&gt;\n  ggplot(aes(x = Agency, y = total_clean_energy)) +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(\n    name = \"Total Amount of Clean Energy Used (kWh)\",\n    breaks = scales::pretty_breaks(n = 6)\n  ) +\n  scale_x_discrete(breaks = function(x) {\n    x[c(TRUE, FALSE)]\n  }, labels = abbreviate, name = \"Agency Name (Abbreviated)\") +\n  theme(axis.text.x = element_text(angle = -45, hjust = 0)) + \n  gghighlight(total_clean_energy &gt; 300000) + \n  labs(title=\"Electricity Used in  Transit Agency Electric Vehicles within States with Primary Clean Energy Sources\")\n\n\n\n\n\n\n\n\n\nNames are abbreviated or removed to save space.\n\n\n🚫🏆🚫 Better Luck Next Time - Most Emissions Per Passenger Trip for a Small Agency\n\n\nCode\nresults |&gt;\n  filter(size == \"Small\") |&gt;\n  slice_max(tot_em_per_mile, n = 1) |&gt;\n  select(Agency,tot_em_per_mile) |&gt;\n  rename(`Total Emissions Per Mile`=tot_em_per_mile) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nAgency\nTotal Emissions Per Mile\n\n\n\n\nAltoona Metro Transit, dba: AMTRAN\n13.00079\n\n\n\n\n\n\n\nThe Altoona Metro Transit gets our Better Luck Next Time award for being the heaviest emitter per mile with 13 pounds of CO2 emitted per mile."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "",
    "text": "I’ve chosen two songs I remember from my teenage years1 as anchor songs. We will return to these later once the our initial exploration of the data set is complete."
  },
  {
    "objectID": "mp03.html#preparing-and-loading-the-data",
    "href": "mp03.html#preparing-and-loading-the-data",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Preparing and Loading the Data",
    "text": "Preparing and Loading the Data\n\n\nCode\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(glue)\nlibrary(purrr)\nlibrary(gt)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(readr)\nlibrary(ggrepel)\n\n\n\nget_spotify_data &lt;- function() {\n  if (!file.exists(\"data/mp03/spotify_data.csv\")) {\n    dir.create(\"data/mp03\", showWarnings = FALSE, recursive = TRUE)\n\n    github_data &lt;- \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\"\n\n    download.file(github_data, \"data/mp03/spotify_data.csv\")\n  }\n\n  spotify_data &lt;- read_csv(\"data/mp03/spotify_data.csv\")\n\n  return(spotify_data)\n}\n\n\nclean_artist_string &lt;- function(x) {\n  x |&gt;\n    str_replace_all(\"\\\\['\", \"\") |&gt;\n    str_replace_all(\"'\\\\]\", \"\") |&gt;\n    str_replace_all(\"[ ]?'\", \"\") |&gt;\n    str_replace_all(\"[ ]*,[ ]*\", \",\")\n}\n\nspotify_data &lt;- get_spotify_data() |&gt;\n  separate_longer_delim(artists, \",\") |&gt;\n  mutate(artist = clean_artist_string(artists)) |&gt;\n  select(-artists)\n\nstrip_spotify_prefix &lt;- function(x) {\n  str_extract(x, \".*:.*:(.*)\", group = 1)\n}\n\nload_playlists &lt;- function(refresh = FALSE) {\n  # Get File Names\n  if (refresh == TRUE) {\n    playlist_data_source &lt;- \"https://api.github.com/repos/DevinOgrady/spotify_million_playlist_dataset/contents/data1\"\n    req &lt;- request(playlist_data_source)\n    resp &lt;- req_perform(req)\n    batch_data &lt;- fromJSON(resp_body_string(resp))\n    files &lt;- batch_data |&gt;\n      bind_rows() |&gt;\n      select(name, download_url)\n    # Init empty list to store playlist values\n    results_list &lt;- list()\n    # Get Files\n    for (i in 1:nrow(files)) {\n      row &lt;- files[i, ]\n      if (!file.exists(glue(\"data/mp03/{row$name}\"))) {\n        download.file(row$download_url, glue(\"data/mp03/{row$name}\"))\n      }\n\n      nested_data &lt;- fromJSON(glue(\"data/mp03/{row$name}\"))$playlists\n      nested_data &lt;- nested_data |&gt;\n        select(-c(duration_ms)) |&gt;\n        unnest(cols = tracks)\n\n      results_list[[i]] &lt;- nested_data\n    }\n\n    playlist_data &lt;- bind_rows(results_list)\n    playlist_data &lt;- playlist_data |&gt;\n      mutate(\n        track_id = strip_spotify_prefix(track_uri),\n        artist_id = strip_spotify_prefix(artist_uri),\n        album_id = strip_spotify_prefix(album_uri)\n      ) |&gt;\n      rename(c(\n        playlist_name = name,\n        playlist_id = pid,\n        playlist_position = pos,\n        playlist_followers = num_followers,\n        duration = duration_ms\n      )) |&gt;\n      select(-c(\n        modified_at,\n        num_edits,\n        description,\n        collaborative,\n        track_uri,\n        artist_uri,\n        album_uri\n      ))\n\n    write.csv(playlist_data, file = \"data/mp03/playlist_data.csv\", sep = \",\", row.names = FALSE)\n  } else {\n    playlist_data &lt;- read_csv(\"data/mp03/playlist_data.csv\")\n  }\n\n  return(playlist_data)\n}\n\n\n\n\nplaylist &lt;- load_playlists()"
  },
  {
    "objectID": "mp03.html#initial-exploration",
    "href": "mp03.html#initial-exploration",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Initial Exploration",
    "text": "Initial Exploration\n\nHow many distinct songs and artists are in the playlist data?\n\n\nCode\nunique_song_count &lt;- playlist |&gt;\n  select(track_id) |&gt;\n  n_distinct()\n\nunique_artist_count &lt;- playlist |&gt;\n  select(artist_id) |&gt;\n  n_distinct()\n\n\nThere are 1200590 unique songs and 173604 distinct artists in the data set.\n\n\nWhat are the five most popular tracks in the playlist data?\n\n\nCode\nplaylist |&gt;\n  group_by(track_id, artist_name, track_name) |&gt;\n  summarise(num_entries = n()) |&gt;\n  ungroup() |&gt;\n  slice_max(num_entries, n = 5) |&gt;\n  select(artist_name, track_name, num_entries) |&gt;\n  rename(\n    Aritst = artist_name,\n    `Song Name` = track_name,\n    `Number of Playlist Appearances` = num_entries\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top Five Songs Featured in the Most Playlists that is not in the Characteristics Dataset\"\n  )\n\n\n\n\n\n\n\n\nTop Five Songs Featured in the Most Playlists that is not in the Characteristics Dataset\n\n\nAritst\nSong Name\nNumber of Playlist Appearances\n\n\n\n\nKendrick Lamar\nHUMBLE.\n13314\n\n\nDrake\nOne Dance\n12179\n\n\nDRAM\nBroccoli (feat. Lil Yachty)\n11845\n\n\nThe Chainsmokers\nCloser\n11656\n\n\nPost Malone\nCongratulations\n11310\n\n\n\n\n\n\n\nKendrick Lamar’s HUMBLE is the most popular song to be put into a playlist. It is featured on over 13,000 playlists.\n\n\n\nWhat is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\n\n\nCode\npaired_down_data &lt;- spotify_data |&gt;\n  select(id, name) |&gt;\n  unique()\n\nanti_join(playlist, paired_down_data, by = join_by(track_id == id)) |&gt;\n  group_by(artist_name, track_name) |&gt;\n  summarise(num_entries = n()) |&gt;\n  ungroup() |&gt;\n  slice_max(num_entries, n = 1) |&gt;\n  rename(\n    `Song Name` = track_name,\n    Artist = artist_name\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Most Popular Song that does not have an Entry in the Characteristics Dataset\"\n  )\n\n\n\n\n\n\n\n\nMost Popular Song that does not have an Entry in the Characteristics Dataset\n\n\nArtist\nSong Name\nnum_entries\n\n\n\n\nDrake\nOne Dance\n12179\n\n\n\n\n\n\n\nThe most popular song in the playlist data that does not have an entry in the Spotify characteristics data is Drake’s One Dance.\n\n\n\nAccording to the song characteristics data, what is the most “danceable” track? How often does it appear in a playlist?\n\n\nCode\nspotify_data |&gt;\n  slice_max(danceability, n = 1) |&gt;\n  select(artist, name) |&gt;\n  rename(\n    Artist = artist,\n    `Song Name` = name\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"The Most Danceable Song\"\n  )\n\n\n\n\n\n\n\n\nThe Most Danceable Song\n\n\nArtist\nSong Name\n\n\n\n\nTone-Loc\nFunky Cold Medina\n\n\n\n\n\n\n\n\n\n\nCode\nplaylist |&gt;\n  filter(track_id == \"5YIF6HSOtHN9HdcE5IPzMe\") |&gt;\n  group_by(track_name) |&gt;\n  summarize(count = n()) |&gt;\n  rename(\n    `Song Name` = track_name,\n    `Number of Playlist Appearances` = count\n  ) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nSong Name\nNumber of Playlist Appearances\n\n\n\n\nFunky Cold Medina\n211\n\n\n\n\n\n\n\nFunky Cold Medina is features in 211 playlists.\n\n\nWhich playlist has the longest average track length?\n\n\nCode\nplaylist |&gt;\n  group_by(playlist_id, playlist_name) |&gt;\n  summarize(avg_song_length_minutes = mean(duration) / 60000) |&gt;\n  ungroup() |&gt;\n  slice_max(avg_song_length_minutes, n = 1) |&gt;\n  rename(\n    `Playlist ID` = playlist_id,\n    `Playlist Name` = playlist_name,\n    `Average Song Duration in Minutes` = avg_song_length_minutes\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\"Playlist with the Most Longest Average Song Duration\")\n\n\n\n\n\n\n\n\nPlaylist with the Most Longest Average Song Duration\n\n\nPlaylist ID\nPlaylist Name\nAverage Song Duration in Minutes\n\n\n\n\n462471\nMixes\n64.47519\n\n\n\n\n\n\n\nThe playlist Mixes has the longest average track length at ~64.5 minutes a track.\n\n\nWhat is the most popular playlist on Spotify?\n\n\nCode\nplaylist |&gt;\n  select(playlist_id, playlist_name, playlist_followers) |&gt;\n  unique() |&gt;\n  slice_max(playlist_followers, n = 1) |&gt;\n  rename(\n    `Playlist ID` = playlist_id,\n    `Playlist Name` = playlist_name,\n    `Total Number of Playlist Followers` = playlist_followers\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\"Playlist with the Most Followers\")\n\n\n\n\n\n\n\n\nPlaylist with the Most Followers\n\n\nPlaylist ID\nPlaylist Name\nTotal Number of Playlist Followers\n\n\n\n\n746359\nBreaking Bad\n53519\n\n\n\n\n\n\n\nUsing the playlist follower count as a measurement for playlist popularity, Breaking Bad is the most popular playlist at 53,519 followers."
  },
  {
    "objectID": "mp03.html#what-are-the-five-most-popular-tracks-in-the-playlist-data",
    "href": "mp03.html#what-are-the-five-most-popular-tracks-in-the-playlist-data",
    "title": "Dancing is Forbidden (Mini Project 3)",
    "section": "",
    "text": "Code\nplaylist |&gt; \n  group_by(track_id,artist_name,track_name) |&gt;\n  summarise(num_entries=n()) |&gt;\n  ungroup() |&gt; \n  slice_max(num_entries,n=5) |&gt; \n  select(artist_name,track_name, num_entries) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nartist_name\ntrack_name\nnum_entries\n\n\n\n\nKendrick Lamar\nHUMBLE.\n13314\n\n\nDrake\nOne Dance\n12179\n\n\nDRAM\nBroccoli (feat. Lil Yachty)\n11845\n\n\nThe Chainsmokers\nCloser\n11656\n\n\nPost Malone\nCongratulations\n11310\n\n\n\n\n\n\n\nCode\n#left_join(five_popular_tracks, spotify_data, by=join_by(track_id==id)) |&gt;"
  },
  {
    "objectID": "mp03.html#what-is-the-most-popular-track-in-the-playlist-data-that-does-not-have-a-corresponding-entry-in-the-song-characteristics-data",
    "href": "mp03.html#what-is-the-most-popular-track-in-the-playlist-data-that-does-not-have-a-corresponding-entry-in-the-song-characteristics-data",
    "title": "Dancing is Forbidden (Mini Project 3)",
    "section": "",
    "text": "Code\npaired_down_data &lt;- spotify_data |&gt;\n  select(id, name) |&gt;\n  unique()\n\nanti_join(playlist, paired_down_data, by=join_by(track_id==id)) |&gt; \n  group_by(track_id,artist_name,track_name) |&gt;\n  summarise(num_entries=n()) |&gt;\n  ungroup() |&gt; \n  slice_max(num_entries,n=1) |&gt;\n  gt()\n\n\n\n\n\n\n\n\ntrack_id\nartist_name\ntrack_name\nnum_entries\n\n\n\n\n1xznGGDReH1oQq0xzbwXa3\nDrake\nOne Dance\n12179"
  },
  {
    "objectID": "mp03.html#according-to-the-song-characteristics-data-what-is-the-most-danceable-track-how-often-does-it-appear-in-a-playlist",
    "href": "mp03.html#according-to-the-song-characteristics-data-what-is-the-most-danceable-track-how-often-does-it-appear-in-a-playlist",
    "title": "Dancing is Forbidden (Mini Project 3)",
    "section": "",
    "text": "Code\nspotify_data |&gt;\n  slice_max(danceability,n=1) |&gt;\n  select ()\n\n\n\n\nCode\nplaylist |&gt; \n  filter(track_id =='5YIF6HSOtHN9HdcE5IPzMe') |&gt; \n  group_by(track_name) |&gt;\n  summarize(count=n()) |&gt;\n  gt()\n\n\nFunky Cold Medina is features in 211 playlists."
  },
  {
    "objectID": "mp03.html#which-playlist-has-the-longest-average-track-length",
    "href": "mp03.html#which-playlist-has-the-longest-average-track-length",
    "title": "Dancing is Forbidden (Mini Project 3)",
    "section": "",
    "text": "Code\nplaylist |&gt;\n  group_by(playlist_id,playlist_name) |&gt;\n  summarize(avg_song_length_minutes = mean(duration)/60000) |&gt;\n  ungroup() |&gt;\n  slice_max(avg_song_length_minutes,n=1) |&gt;\n  gt()\n\n\nThe playlist mixes has the longest average track lenght at ~64.5 minutes a track."
  },
  {
    "objectID": "mp03.html#what-is-the-most-popular-playlist-on-spotify",
    "href": "mp03.html#what-is-the-most-popular-playlist-on-spotify",
    "title": "Dancing is Forbidden (Mini Project 3)",
    "section": "",
    "text": "Code\nplaylist |&gt; \n  select(playlist_id,playlist_name,playlist_followers) |&gt;\n  unique() |&gt;\n  slice_max(playlist_followers,n=1) |&gt; \n  gt()\n\n\nUsing the playlist follower count as a measurement for playlist popularity, Breaking Bad is the most popular playlist at 53,519 followers. # Popularity Characteristics\n\n\nCode\njoined_data &lt;- inner_join(playlist, spotify_data, by=join_by(track_id==id), multiple=\"first\") \n\n\nindex &lt;- c(0,1,2,3,4,5,6,7,8,9,10,11)\nkey_str &lt;-c(\"C\",\"C#\",\"D\",\"D#\",\"E\",\"F\",\"F#\",\"G\",\"G#\",\"A\",\"A#\",\"B\")\nkey_mapping &lt;-data.frame( index,key_str  )\n#https://en.wikipedia.org/wiki/Chromatic_scale\n\njoined_data &lt;- left_join(joined_data,key_mapping, by=join_by(key==index))"
  },
  {
    "objectID": "mp03.html#is-the-popularity-column-correlated-with-the-number-of-playlist-appearances-if-so-to-what-degree",
    "href": "mp03.html#is-the-popularity-column-correlated-with-the-number-of-playlist-appearances-if-so-to-what-degree",
    "title": "Dancing is Forbidden (Mini Project 3)",
    "section": "",
    "text": "We will define popularity as having a popularity score of at least 65.\n\n\nCode\njoined_data |&gt;\n  group_by(track_id, track_name, popularity) |&gt;\n  summarize(n_playlist_appearances=n()) |&gt; \n  ungroup() |&gt;\n  select(-c(track_id,track_name)) |&gt; \n  ggplot(aes(x=popularity, y=n_playlist_appearances)) + \n  geom_hex() + \n  geom_vline(xintercept=65,\n             linetype=\"dashed\",\n             color='red')\n\n\nBased on the chart, we see that a song is more likely to be in over 5,000 playlists if it has a popularity score of at least 65 compared to songs with popularity scores of less than 65.\n\n\n\n\nCode\n#|label: task_5_2_actual\n\nspotify_data |&gt;\n  select(id,popularity) |&gt; \n  group_by(id) |&gt; \n  summarise(avg_pop = mean(popularity)) |&gt; \n  ggplot(aes(x=avg_pop)) + geom_histogram()\n\n\nCode\n# define popular as being in the 75%+ percentile of the ranking \nsong_only &lt;- spotify_data |&gt; select(id,popularity) |&gt; unique() \nquantile(song_only$popularity)\n\n\n\n\nCode\npopularity_check &lt;- joined_data |&gt;\n  select(track_id,year,popularity) |&gt;\n  unique() |&gt;\n  filter(popularity&gt;=65) |&gt;\n  group_by(year) |&gt; \n  summarize(count=n())|&gt;\n  ungroup() \n\npopularity_check|&gt;\n  ggplot(aes(x=year,y=count)) +\n  geom_bar(stat='identity')\n\n\nWe can see that the most popular songs were released in the year 2017, with 502 tracks of at least a score of 65."
  },
  {
    "objectID": "mp03.html#in-what-year-were-the-most-popular-songs-released",
    "href": "mp03.html#in-what-year-were-the-most-popular-songs-released",
    "title": "Dancing is Forbidden (Mini Project 3)",
    "section": "",
    "text": "Code\n#|label: task_5_2_actual\n\nspotify_data |&gt;\n  select(id,popularity) |&gt; \n  group_by(id) |&gt; \n  summarise(avg_pop = mean(popularity)) |&gt; \n  ggplot(aes(x=avg_pop)) + geom_histogram()\n\n\nCode\n# define popular as being in the 75%+ percentile of the ranking \nsong_only &lt;- spotify_data |&gt; select(id,popularity) |&gt; unique() \nquantile(song_only$popularity)\n\n\n\n\nCode\npopularity_check &lt;- joined_data |&gt;\n  select(track_id,year,popularity) |&gt;\n  unique() |&gt;\n  filter(popularity&gt;=65) |&gt;\n  group_by(year) |&gt; \n  summarize(count=n())|&gt;\n  ungroup() \n\npopularity_check|&gt;\n  ggplot(aes(x=year,y=count)) +\n  geom_bar(stat='identity')\n\n\nWe can see that the most popular songs were released in the year 2017, with 502 tracks of at least a score of 65."
  },
  {
    "objectID": "mp03.html#in-what-year-did-danceability-peak",
    "href": "mp03.html#in-what-year-did-danceability-peak",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "In what year did danceability peak?",
    "text": "In what year did danceability peak?\n\n\nCode\ndance_dance_dance_til_youre_dead &lt;- joined_data |&gt;\n  select(track_id, year, danceability) |&gt;\n  unique() |&gt;\n  group_by(year) |&gt;\n  summarize(average_danceability = mean(danceability)) |&gt;\n  ungroup()\n\nheads_will_roll &lt;- dance_dance_dance_til_youre_dead |&gt;\n  slice_max(average_danceability, n = 1) |&gt;\n  select(year)\n\ndance_dance_dance_til_youre_dead |&gt;\n  rename(\n    `Year` = year,\n    `Average Danceability` = average_danceability\n  ) |&gt;\n  ggplot(aes(x = `Year`, y = `Average Danceability`)) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"Average Danceability by Year of Release\") +\n  scale_fill_manual(values = c(\"steelblue\")) +\n  coord_cartesian(ylim = c(0.4, 0.8)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nAverage danceability peaked in the year 1929."
  },
  {
    "objectID": "mp03.html#which-decade-is-most-represented-on-user-playlists",
    "href": "mp03.html#which-decade-is-most-represented-on-user-playlists",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Which decade is most represented on user playlists?",
    "text": "Which decade is most represented on user playlists?\n\n\nCode\njoined_data |&gt;\n  select(playlist_id, track_id, year, ) |&gt;\n  mutate(decade_mapper = (year %/% 10) * 10) |&gt;\n  unique() |&gt;\n  group_by(decade_mapper) |&gt;\n  summarize(count = n()) |&gt;\n  ungroup() |&gt;\n  rename(\n    `Decade` = decade_mapper,\n    `Number of Songs` = count\n  ) |&gt;\n  ggplot(aes(x = `Decade`, y = `Number of Songs`)) +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(labels = scales::label_comma()) +\n  ggtitle(\"Number of Songs in Playlists by Decade of Release\") +\n  scale_fill_manual(values = c(\"steelblue\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe 2010’s are the most heavily represented in the playlist data."
  },
  {
    "objectID": "mp03.html#what-musical-keys-are-the-most-represented",
    "href": "mp03.html#what-musical-keys-are-the-most-represented",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "What musical keys are the most represented?",
    "text": "What musical keys are the most represented?\n\n\nCode\nkey_data &lt;- spotify_data |&gt;\n  select(id, key_str) |&gt;\n  unique() |&gt;\n  group_by(key_str) |&gt;\n  summarize(count = n())\n\nkey_data_2 &lt;- key_data |&gt;\n  mutate(\n    csum = rev(cumsum(rev(count))),\n    pos = count / 2 + lead(csum, 1),\n    pos = if_else(is.na(pos), count / 2, pos)\n  )\n\nkey_data |&gt;\n  ggplot(aes(x = \"\", y = count, fill = key_str)) +\n  geom_bar(stat = \"identity\", width = 10) +\n  coord_polar(\"y\", start = 0) +\n  scale_y_continuous(breaks = key_data_2$pos, labels = key_data_2$key_str) +\n  geom_text(aes(label = count),\n    position = position_stack(vjust = 0.5), label.size = 0.1\n  ) +\n  scale_fill_manual(values = c(\n    \"#8dd3c7\", \"#ffffb3\", \"#bebada\", \"#fb8072\", \"#80b1d3\",\n    \"#fdb462\", \"#b3de69\", \"#fccde5\", \"#d9d9d9\", \"#bc80bd\",\n    \"#ccebc5\", \"#ffed6f\"\n  )) +\n  theme_void() +\n  theme(axis.text = element_text(size = 15), legend.position = \"none\") +\n  ggtitle(\"Number of Songs in Characteristics Data by Key of Song \")\n\n\n\n\n\n\n\n\n\nThe most common key is C with 22,499 unique tracks."
  },
  {
    "objectID": "mp03.html#what-are-the-most-popular-track-lengths",
    "href": "mp03.html#what-are-the-most-popular-track-lengths",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "What are the most popular track lengths?",
    "text": "What are the most popular track lengths?\n\n\nCode\njoined_data |&gt;\n  select(track_id, duration) |&gt;\n  unique() |&gt;\n  # convert miliseconds to minutes\n  mutate(`Duration in Minutes` = duration / 60000) |&gt;\n  ggplot(aes(x = `Duration in Minutes`)) +\n  geom_histogram() +\n  xlim(0, 10) +\n  ggtitle(\"Number of Songs by Track Length in Minutes\") +\n  ylab(\"Number of Songs\") +\n  scale_fill_manual(values = c(\"steelblue\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe average duration of a song on the playlist is 3.75 minutes or 3 minutes and 45 seconds. We have limited the x-axis so that the max length is 10 minutes in this view."
  },
  {
    "objectID": "mp03.html#is-there-a-relationship-between-danceability-and-tempo",
    "href": "mp03.html#is-there-a-relationship-between-danceability-and-tempo",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Is there a relationship between danceability and tempo?",
    "text": "Is there a relationship between danceability and tempo?\n\n\nCode\nspotify_data |&gt;\n  mutate(decade_mapper = (year %/% 10) * 10) |&gt;\n  select(id, decade_mapper, tempo, danceability) |&gt;\n  unique() |&gt;\n  rename(\n    `Tempo` = tempo,\n    `Danceability` = danceability\n  ) |&gt;\n  ggplot(aes(x = `Tempo`, y = `Danceability`)) +\n  geom_hex() +\n  guides(fill = guide_legend(\"Measurement\\nDensity\")) +\n  facet_wrap(vars(decade_mapper)) +\n  theme_bw() +\n  ggtitle(\"Danceability Compared to Tempo over Several Decades\")\n\n\n\n\n\n\n\n\n\nThere’s a noticeable bump in dancibility vs. tempo around 125 for most of the decades plotted. The exact “center of mass” seems to be changing throughout the decades, but people seem to be pushed to the dance floor consistently more around 125 bpm."
  },
  {
    "objectID": "mp03.html#does-acousticness-matter-for-popularity",
    "href": "mp03.html#does-acousticness-matter-for-popularity",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Does acousticness matter for popularity?",
    "text": "Does acousticness matter for popularity?\n\n\nCode\nspotify_data |&gt;\n  mutate(decade_mapper = (year %/% 10) * 10) |&gt;\n  select(id, decade_mapper, acousticness, popularity) |&gt;\n  unique() |&gt;\n  rename(\n    `Acousticness` = acousticness,\n    `Popularity` = popularity,\n  ) |&gt;\n  ggplot(aes(x = `Acousticness`, y = `Popularity`)) +\n  geom_hex() +\n  guides(fill = guide_legend(\"Measurement\\nDensity\")) +\n  facet_wrap(vars(decade_mapper)) +\n  theme_bw() +\n  ggtitle(\"Acousticness Compared to Popularity over Several Decades\")\n\n\n\n\n\n\n\n\n\nWe can see an interesting trend in the data when we control for the decade. Acousticness matters for popularity for songs up until the 50’s, but from the 60’s onward it does not matter. We can also see how the popularity floor raises from the 60’s onward. Newer music is the most popular on the platform."
  },
  {
    "objectID": "mp03.html#footnotes",
    "href": "mp03.html#footnotes",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOnly after doing the analysis did I realize that they were both used in iPod commercials!↩︎"
  },
  {
    "objectID": "mp03.html#popularity-characteristics",
    "href": "mp03.html#popularity-characteristics",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Popularity Characteristics",
    "text": "Popularity Characteristics\n\n\nCode\njoined_data &lt;- inner_join(playlist, spotify_data, by = join_by(track_id == id), multiple = \"first\")\n\n\nindex &lt;- c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nkey_str &lt;- c(\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\")\nkey_mapping &lt;- data.frame(index, key_str)\n# https://en.wikipedia.org/wiki/Chromatic_scale\n\njoined_data &lt;- left_join(joined_data, key_mapping, by = join_by(key == index))\nspotify_data &lt;- left_join(spotify_data, key_mapping, by = join_by(key == index))\n\n\n\nIs the popularity column correlated with the number of playlist appearances? If so, to what degree?\nWe will define popularity as having a popularity score of at least 65.\n\n\n\n\n\n\n\n\n\nBased on the chart, we see that a song is more likely to be in over 5,000 playlists if it has a popularity score of at least 65 compared to songs with popularity scores of less than 65.\nIt is interesting to note that the majority of the songs in the joined data set appear to have a popularity between 25 and 65 but also are not featured in that many playlists. This may be because of the popularity differences between the singles of an album and the rest of the songs on an album. It is also likely that older songs are less popular and featured in fewer playlists across the board.\n\n\nIn what year were the most popular songs released?\n\n\nCode\npopularity_check &lt;- joined_data |&gt;\n  select(track_id, year, popularity) |&gt;\n  unique() |&gt;\n  filter(popularity &gt;= 65) |&gt;\n  group_by(year) |&gt;\n  summarize(count = n()) |&gt;\n  ungroup()\n\npopularity_check |&gt;\n  rename(\n    `Year` = year,\n    `Number of Songs over the Popularity Threshold` = count\n  ) |&gt;\n  ggplot(aes(x = `Year`, y = `Number of Songs over the Popularity Threshold`)) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"Popular Song Releases by Year Released\") +\n  scale_fill_manual(values = c(\"steelblue\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nWe can see that the most popular songs were released in the year 2017, with 502 tracks of at least a score of 65."
  },
  {
    "objectID": "mp03.html#code-used-to-analyze-the-anchor-songs",
    "href": "mp03.html#code-used-to-analyze-the-anchor-songs",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Code used to analyze the anchor songs",
    "text": "Code used to analyze the anchor songs\n\n\nCode\n# anchor song Bruises\n\n# isolate song stats\nbruises &lt;- spotify_data |&gt; filter(id == \"4mdyVTV7Tr5YDFnD2kvSM4\")\nbruises &lt;- left_join(bruises, key_mapping, by = join_by(key == index))\n\n# same key and tempo\nchairlist_key_and_tempo &lt;- spotify_data |&gt;\n  filter(key == 0, tempo &gt;= 185, tempo &lt;= 186) |&gt;\n  select(artist, name, id) |&gt;\n  unique()\n\n\n# same artist\nchairlift &lt;- spotify_data |&gt; filter(artist == \"Chairlift\", id != \"4mdyVTV7Tr5YDFnD2kvSM4\")\n\n\n\n# Similar valence and year\nsimilar_valaence_and_year &lt;- spotify_data |&gt;\n  filter(valence &gt;= 0.8, valence &lt;= .9, year == 2008) |&gt;\n  select(artist, name, id) |&gt;\n  unique()\n\n\n# similar songs from other playlists\nbruises_playlists &lt;- joined_data |&gt;\n  filter(track_id == \"4mdyVTV7Tr5YDFnD2kvSM4\") |&gt;\n  select(playlist_id) |&gt;\n  unique()\n\n\nbruises_similar_playlists &lt;- joined_data |&gt;\n  filter(playlist_id %in% bruises_playlists$playlist_id) |&gt;\n  arrange(desc(playlist_followers)) |&gt;\n  filter(artist_name != \"Chairlift\") |&gt;\n  select(name, artist, track_id) |&gt;\n  unique()\n\n\n\n\nCode\n# Anchor Song\n# Feist - 1234 2CzWeyC9zlDpIOZPUUKrBW\n\n# isolate song stats\nfeist_1234 &lt;- spotify_data |&gt; filter(id == \"2CzWeyC9zlDpIOZPUUKrBW\")\nfeist_1234 &lt;- left_join(feist_1234, key_mapping, by = join_by(key == index))\n\n\n\n# same key and tempo\nsame_key_and_tempo_feist &lt;- spotify_data |&gt;\n  filter(key == 2, tempo &gt;= 105, tempo &lt;= 115) |&gt;\n  arrange(desc(popularity)) |&gt;\n  select(id, artist, name, popularity) |&gt;\n  unique()\n\n# same artist\nfiest &lt;- spotify_data |&gt; filter(artist == \"Feist\", id != \"2CzWeyC9zlDpIOZPUUKrBW\")\n\n\n# matching year with similar tempo\nsame_year_and_tempo_feist &lt;- spotify_data |&gt;\n  filter(year == 2007) |&gt;\n  filter(tempo &gt;= 105, tempo &lt;= 115) |&gt;\n  select(id, name, artist)\n\n\nsame_year_and_energy_feist &lt;- spotify_data |&gt;\n  filter(energy &gt;= 0.4, energy &lt;= 0.5, year == 2007) |&gt;\n  select(artist, name, popularity)\n\n\nfeist_1234_playlists &lt;- joined_data |&gt;\n  filter(track_id == \"2CzWeyC9zlDpIOZPUUKrBW\") |&gt;\n  select(playlist_id) |&gt;\n  unique()\n\n\nfeist_playlists_similar &lt;- joined_data |&gt;\n  filter(playlist_id %in% feist_1234_playlists$playlist_id) |&gt;\n  filter(artist_name != \"Feist\") |&gt;\n  select(artist_name, track_name, track_id) |&gt;\n  unique()\n\n\nThe above code was run to select a large batch of candidates. Following this, the dataframes were manually inspected to refine the candidates."
  },
  {
    "objectID": "mp03.html#playlist-candidates",
    "href": "mp03.html#playlist-candidates",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Playlist candidates",
    "text": "Playlist candidates\n\n\nCode\ncandidates &lt;- spotify_data |&gt;\n  filter(id %in% c(\n    \"2KjJhGtseXQBI5Cg6ZZdc9\",\n    \"221bl0jrSfJ1LMgUw36zRJ\",\n    \"4VNWVzVj6QnNwasZBjPGdu\",\n    \"0aqRkWPAL9BGCvvdSiXaE9\",\n    \"4mdyVTV7Tr5YDFnD2kvSM4\",\n    \"4g3iTV4Pr70I4o2xMJCZGL\",\n    \"1595LW73XBxkRk2ciQOHfr\",\n    \"6tN0t7Y1bFI1ZU98Ou8Z3i\",\n    \"2KjJhGtseXQBI5Cg6ZZdc9\",\n    \"1I7zHEdDx8Ny5RxzYPqsU2\",\n    \"5IRWgQiN0c2i4U97iE3OoY\",\n    \"3OeUlriM0EZHdWleJtjoVr\",\n    \"0GO8y8jQk1PkHzS31d699N\",\n    \"6cdslY3YKjh7pImxFhSBVG\",\n    \"2CzWeyC9zlDpIOZPUUKrBW\",\n    \"6QPhpvO1pWDS91EsEmzsbc\",\n    \"43YJuMTeTEV5FyjVfBqwyf\",\n    \"51R5mPcJjOnfv9lKY1u5sW\",\n    \"6Hmj7SrLRbreLVfVS7mV1S\",\n    \"0VXp4zDspR993l2hIKW17g\",\n    \"52PuLmsxTDOI2WAqpzYjoT\",\n    \"6Ge2Ob69SEZ2l78e3IE0Aw\",\n    \"1NeLwFETswx8Fzxl2AFl91\",\n    \"6AKSrodq9kU9hmTLEeWFFr\",\n    \"3kZC0ZmFWrEHdUCmUqlvgZ\",\n    \"4Y3gLYlV5rD3fhirhWKtei\",\n    \"2RlgNHKcydI9sayD2Df2xp\",\n    \"1YLJVmuzeM2YSUkCCaTNUB\",\n    \"ghgwo0BTPm329zGMQRfn7\",\n    \"5FFQbvn7055P1DvgJDdCBP\"\n  )) |&gt;\n  select(\n    id, name, artist, year, acousticness, danceability, energy,\n    instrumentalness, liveness, loudness, speechiness, tempo, valence,\n    popularity, key_str\n  )\n\ncandidates |&gt;\n  select(name, artist, year) |&gt;\n  rename(\n    `Artist` = artist,\n    `Song Name` = name,\n    Year = year\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Manually Selected Candidate Songs\"\n  )\n\n\n\n\n\n\n\n\nManually Selected Candidate Songs\n\n\nSong Name\nArtist\nYear\n\n\n\n\nO Valencia!\nThe Decemberists\n2006\n\n\nShut Up and Let Me Go\nThe Ting Tings\n2008\n\n\nI Found A Reason\nCat Power\n2000\n\n\nMy Moon My Man\nFeist\n2007\n\n\nLloyd, I'm Ready To Be Heartbroken\nCamera Obscura\n2006\n\n\nCalifornia\nDelta Spirit\n2012\n\n\nNicest Thing\nKate Nash\n2007\n\n\nCe jeu\nYelle\n2007\n\n\nSettle Down\nKimbra\n2011\n\n\nI Belong in Your Arms\nChairlift\n2012\n\n\nReelin' In The Years\nSteely Dan\n1972\n\n\nMr. Blue Sky\nElectric Light Orchestra\n1977\n\n\nGot My Mind Set On You - 2004 Mix\nGeorge Harrison\n1987\n\n\nSomething About Us\nDaft Punk\n2001\n\n\nWake Up\nArcade Fire\n2004\n\n\nI Will Follow You into the Dark\nDeath Cab for Cutie\n2005\n\n\n1234\nFeist\n2007\n\n\nA-Punk\nVampire Weekend\n2008\n\n\nDog Days Are Over\nFlorence + The Machine\n2009\n\n\nTongue Tied\nGrouplove\n2011\n\n\nHeart of Gold - 2017 Remaster\nNeil Young\n1977\n\n\nCrush\nDave Matthews Band\n1998\n\n\nNever Meant\nAmerican Football\n1999\n\n\nAnthems For A Seventeen Year-Old Girl\nBroken Social Scene\n2003\n\n\nBruises\nChairlift\n2008\n\n\nWith Or Without You\nU2\n2006\n\n\nPlease Call Me, Baby\nTom Waits\n1974\n\n\nFutile Devices\nSufjan Stevens\n2010\n\n\n\n\n\n\n\nHere we have our list of candidates. Let us see what can be done to refine the list further.\n\n\nCode\ncandidates |&gt;\n  select(id, name, energy, tempo) |&gt;\n  ggplot(aes(x = energy, y = tempo)) +\n  geom_point() +\n  geom_label_repel(\n    data = subset(candidates, id %in% c(\"2CzWeyC9zlDpIOZPUUKrBW\", \"4mdyVTV7Tr5YDFnD2kvSM4\")),\n    aes(label = name),\n    box.padding = 0.35,\n    point.padding = 0.5,\n    segment.color = \"grey50\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nBased on this, we can remove the songs with an energy score that is less than that of ‘1234’. We then get the below list of songs, but let’s see if we can refine this even more.\n\n\nCode\nenergy_filter &lt;- candidates |&gt;\n  filter(id == \"2CzWeyC9zlDpIOZPUUKrBW\") |&gt;\n  select(energy) |&gt;\n  pluck(1)\n\ncandidates &lt;- candidates |&gt;\n  filter(energy &gt;= 0.484)\n\ncandidates |&gt;\n  select(name, artist, year) |&gt;\n  rename(\n    `Artist` = artist,\n    `Song Name` = name,\n    Year = year\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Refined Candidate Songs\"\n  )\n\n\n\n\n\n\n\n\nRefined Candidate Songs\n\n\nSong Name\nArtist\nYear\n\n\n\n\nO Valencia!\nThe Decemberists\n2006\n\n\nShut Up and Let Me Go\nThe Ting Tings\n2008\n\n\nMy Moon My Man\nFeist\n2007\n\n\nLloyd, I'm Ready To Be Heartbroken\nCamera Obscura\n2006\n\n\nCalifornia\nDelta Spirit\n2012\n\n\nCe jeu\nYelle\n2007\n\n\nSettle Down\nKimbra\n2011\n\n\nI Belong in Your Arms\nChairlift\n2012\n\n\nReelin' In The Years\nSteely Dan\n1972\n\n\nGot My Mind Set On You - 2004 Mix\nGeorge Harrison\n1987\n\n\nWake Up\nArcade Fire\n2004\n\n\n1234\nFeist\n2007\n\n\nA-Punk\nVampire Weekend\n2008\n\n\nDog Days Are Over\nFlorence + The Machine\n2009\n\n\nTongue Tied\nGrouplove\n2011\n\n\nCrush\nDave Matthews Band\n1998\n\n\nNever Meant\nAmerican Football\n1999\n\n\nBruises\nChairlift\n2008\n\n\nWith Or Without You\nU2\n2006\n\n\n\n\n\n\n\n\n\nCode\ncandidates |&gt;\n  select(id, name, valence, danceability) |&gt;\n  ggplot(aes(x = valence, y = danceability)) +\n  geom_point() +\n  geom_label_repel(\n    data = subset(candidates, id %in% c(\"2CzWeyC9zlDpIOZPUUKrBW\", \"4mdyVTV7Tr5YDFnD2kvSM4\")),\n    aes(label = name),\n    box.padding = 0.6,\n    point.padding = 0.5,\n    segment.color = \"grey50\"\n  ) +\n  geom_segment(x = 0, y = 1, xend = 1, yend = 0, color = \"red\", linetype = \"dashed\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nFrom this chart, we can see a distinct split in the data into two clusters (while keeping our anchor songs in the playlist) with the function \\(y=1-x\\) with respect to the danceability and valence of a song. We will filter out anything below the red dashed line. This is make sure that the playlist optimized happy\nAs a final step, we will organize our songs by the Spotify energy metric, so that our playlist becomes more exciting before reaching the crescendo.\n\n\nCode\nfinal_playlist &lt;- candidates |&gt;\n  mutate(side_of_line = ifelse(danceability &gt; 1 - valence, \"above\", \"below\")) |&gt;\n  filter(side_of_line == \"above\") |&gt;\n  arrange(energy)\n\nfinal_playlist |&gt;\n  select(name, artist, year, popularity) |&gt;\n  rename(\n    `Artist` = artist,\n    `Song Name` = name,\n    Year = year,\n    Popularity = popularity\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"The Ultimate Playlist\"\n  )\n\n\n\n\n\n\n\n\nThe Ultimate Playlist\n\n\nSong Name\nArtist\nYear\nPopularity\n\n\n\n\n1234\nFeist\n2007\n60\n\n\nMy Moon My Man\nFeist\n2007\n42\n\n\nBruises\nChairlift\n2008\n56\n\n\nCe jeu\nYelle\n2007\n45\n\n\nSettle Down\nKimbra\n2011\n47\n\n\nCrush\nDave Matthews Band\n1998\n54\n\n\nReelin' In The Years\nSteely Dan\n1972\n72\n\n\nI Belong in Your Arms\nChairlift\n2012\n47\n\n\nA-Punk\nVampire Weekend\n2008\n66\n\n\nGot My Mind Set On You - 2004 Mix\nGeorge Harrison\n1987\n70\n\n\nShut Up and Let Me Go\nThe Ting Tings\n2008\n43\n\n\n\n\n\n\n\nAbove, we have our final playlist."
  },
  {
    "objectID": "mp03.html#by-feist-1",
    "href": "mp03.html#by-feist-1",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "1234 by Feist",
    "text": "1234 by Feist"
  },
  {
    "objectID": "mp03.html#my-moon-my-man-by-feist",
    "href": "mp03.html#my-moon-my-man-by-feist",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "My Moon My Man by Feist",
    "text": "My Moon My Man by Feist"
  },
  {
    "objectID": "mp03.html#bruises-by-chairlift-1",
    "href": "mp03.html#bruises-by-chairlift-1",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Bruises by Chairlift",
    "text": "Bruises by Chairlift"
  },
  {
    "objectID": "mp03.html#ce-jeu-by-yelle",
    "href": "mp03.html#ce-jeu-by-yelle",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Ce jeu by Yelle",
    "text": "Ce jeu by Yelle"
  },
  {
    "objectID": "mp03.html#settle-down-by-kimbra",
    "href": "mp03.html#settle-down-by-kimbra",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Settle Down by Kimbra",
    "text": "Settle Down by Kimbra"
  },
  {
    "objectID": "mp03.html#crush-by-dave-matthews-band",
    "href": "mp03.html#crush-by-dave-matthews-band",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Crush by Dave Matthews Band",
    "text": "Crush by Dave Matthews Band\n ## Reelin’ in the Years by Steely Dan"
  },
  {
    "objectID": "mp03.html#i-belong-in-your-arms-by-chairlift",
    "href": "mp03.html#i-belong-in-your-arms-by-chairlift",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "I Belong in Your Arms by Chairlift",
    "text": "I Belong in Your Arms by Chairlift"
  },
  {
    "objectID": "mp03.html#a-punk-by-vampire-weekend",
    "href": "mp03.html#a-punk-by-vampire-weekend",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "A-Punk by Vampire Weekend",
    "text": "A-Punk by Vampire Weekend"
  },
  {
    "objectID": "mp03.html#got-my-mind-set-on-you-by-george-harrison",
    "href": "mp03.html#got-my-mind-set-on-you-by-george-harrison",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Got My Mind Set On You by George Harrison",
    "text": "Got My Mind Set On You by George Harrison"
  },
  {
    "objectID": "mp03.html#shut-up-and-let-me-go-by-the-ting-tings",
    "href": "mp03.html#shut-up-and-let-me-go-by-the-ting-tings",
    "title": "Nostalgia Exploration (Mini Project 3)",
    "section": "Shut Up and Let Me Go by the Ting Tings",
    "text": "Shut Up and Let Me Go by the Ting Tings"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini Project 4",
    "section": "",
    "text": "Code\nlibrary(janitor)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(httr2)\nlibrary(sf)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(tigris)"
  },
  {
    "objectID": "mp04.html#county-shape-file-data",
    "href": "mp04.html#county-shape-file-data",
    "title": "Mini Project 4",
    "section": "County Shape File Data",
    "text": "County Shape File Data\n\n\nCode\nget_shape_data &lt;- function(refresh = FALSE) {\n  if (refresh == TRUE) {\n    if (!file.exists(\"data/mp04/cb_2023_us_all_20m.zip\")) {\n      dir.create(\"data/mp04\", showWarnings = FALSE, recursive = TRUE)\n\n      shape_data &lt;- \"https://www2.census.gov/geo/tiger/GENZ2023/shp/cb_2023_us_all_500k.zip\"\n\n      download.file(shape_data, \"data/mp04/cb_2023_us_all_500k.zip/cb_2023_us_county_500k/cb_2023_us_county_500k.shp\")\n      unzip(\"data/mp04/cb_2023_us_all_500k.zip\", exdir = \"data/mp04\")\n      unzip(\"data/mp04/cb_2023_us_all_500k/cb_2023_us_county_500k.zip\",exdir=\"data/mp04/cb_2023_us_all_500k/\")\n    }\n  }\n  counties &lt;- read_sf(\"data/mp04/cb_2023_us_all_500k/cb_2023_us_county_500k/cb_2023_us_county_500k.shp\")\n}\ncounties &lt;- get_shape_data() \n\n\ngeo_shifted &lt;- shift_geometry(\n  counties,\n  position = \"below\", # other option: \"outside\"\n  preserve_area = FALSE\n  )|&gt;\n  mutate(center = st_centroid(geometry))"
  },
  {
    "objectID": "mp04.html#wikipedia-data",
    "href": "mp04.html#wikipedia-data",
    "title": "Mini Project 4",
    "section": "Wikipedia Data",
    "text": "Wikipedia Data\nBelow is the code used to retreive the State and County election results from wikipedia.\n\n\nCode\nget_wiki_data_by_state &lt;- function(state, year, refresh = FALSE) {\n  if (!state == \"Washington\") {\n    clean_state &lt;- state |&gt; \n      str_replace_all(\" \", \"_\")\n  } else {\n    clean_state &lt;- \"Washington_(state)\"\n  }\n\n\n  cache_file_path &lt;- paste(\"data/mp04/States/\", clean_state, \"_\", year, \".html\", sep = \"\")\n\n  if (refresh == TRUE) {\n    url &lt;- paste(\"https://en.wikipedia.org/wiki/\", year, \"_United_States_presidential_election_in_\", clean_state, sep = \"\")\n    download.file(url, cache_file_path)\n  }\n\n  if (!clean_state %in% c(\"Ohio\", \"Pennsylvania\", \"Utah\")) {\n    df &lt;- read_html(cache_file_path) |&gt;\n      html_elements(\"table\") |&gt;\n      html_table(header = TRUE) |&gt;\n      Filter(\n        \\(x) ((\"Parish\" %in% str_sub(colnames(x)) |\n          (\"County\" %in% str_sub(colnames(x), 1, 6))) &\n          (!\"Firstalignment\" %in% (colnames(x))) &\n          (!\"City\" %in% colnames(x)) &\n          (!\"County.elections\" %in% (colnames(x))) &\n          (!\"Jo JorgensenLibertarian\" %in% colnames(x))),\n        x = _\n      ) |&gt;\n      data.frame() |&gt;\n      mutate(state = clean_state) |&gt;\n      tail(-1)\n  } else {\n    df &lt;- read_html(cache_file_path) |&gt;\n      html_elements(\"table\") |&gt;\n      html_table(header = TRUE) |&gt;\n      Filter(\n        \\(x) (\n          (\"County\" %in% str_sub(colnames(x), 1, 6)) &\n            ((\"Jo JorgensenLibertarian\" %in% colnames(x)) |\n              (\"Various candidatesOther parties\" %in% colnames(x)))),\n        x = _\n      ) |&gt;\n      data.frame() |&gt;\n      mutate(state = clean_state) |&gt;\n      tail(-1)\n  }\n\n\n  if (\"County.elections\" %in% colnames(df)) {\n    df &lt;- subset(df, select = -c(`County.elections`))\n  }\n  if (\"County.executive.elections\" %in% colnames(df)) {\n    df &lt;- subset(df, select = -c(`County.executive.elections`))\n  }\n\n\n  total_cleaner_index &lt;- ncol(df) - 1\n\n  if (\"Joe.BidenDFL\" %in% colnames(df)) {\n    df$`Joe.BidenDemocratic` &lt;- df$`Joe.BidenDFL`\n    tmp &lt;- ncol(df) - 2\n  }\n\n  if (\"Joe.BidenDemocratic.NPL\" %in% colnames(df)) {\n    df$`Joe.BidenDemocratic` &lt;- df$`Joe.BidenDemocratic.NPL`\n    tmp &lt;- ncol(df) - 2\n  }\n\n  if (\"Kamala.HarrisDFL\" %in% colnames(df)) {\n    df$`Kamala.HarrisDemocratic` &lt;- df$`Kamala.HarrisDFL`\n    tmp &lt;- ncol(df) - 2\n  }\n\n  if (\"Kamala.HarrisDemocratic.NPL\" %in% colnames(df)) {\n    df$`Kamala.HarrisDemocratic` &lt;- df$`Kamala.HarrisDemocratic.NPL`\n    tmp &lt;- ncol(df) - 2\n  }\n\n  colnames(df)[1] &lt;- \"County\"\n  colnames(df)[total_cleaner_index] &lt;- \"Total\"\n\n  if (year == \"2020\") {\n    df &lt;- df |&gt;\n      select(c(\n        County,\n        Donald.TrumpRepublican,\n        Joe.BidenDemocratic,\n        Margin,\n        Total,\n        state\n      ))\n  } else {\n    df &lt;- df |&gt;\n      select(c(\n        County,\n        Donald.TrumpRepublican, \n        Kamala.HarrisDemocratic,\n        Margin,\n        Total,\n        state\n      ))\n  }\n  write.csv(df, paste(\"data/mp04/States/\", clean_state, \"_\", year, \"_dataframe.csv\", sep = \"\"))\n\n  return(df)\n}\n\n\nWe have to filter out Alaska from the data as the Wikipedia data is not present at the county-equivalent level. Connecticut is not properly in the Census data format at the county level and as such will be excluded from this analysis.\n\n\nCode\nstate.name |&gt;\n  Filter(\\(x) !\"Alaska\" %in% x, x = _) |&gt;\n  map(\\(x) get_wiki_data_by_state(x, \"2024\"))\n\nstate.name |&gt;\n  Filter(\\(x) !\"Alaska\" %in% x, x = _) |&gt;\n  map(\\(x) get_wiki_data_by_state(x, \"2020\"))\n\n\nNow we will read the data into data frames.\n\n\nCode\nfiles_2024 &lt;- list.files(path = \"data/mp04/States/\", pattern = \"2024_dataframe.csv\", full.names = TRUE)\ndf_2024 &lt;- read_csv(files_2024) |&gt;\n  bind_rows() |&gt; \n  filter(!`County`=='Totals') |&gt;\n  filter(!`County`=='Total') |&gt;\n  select(-c(`...1`)) |&gt; \n  mutate(State =  case_when(state=='Washington_(state)' ~ 'Washington',\n                              TRUE  ~ str_replace_all(state,\"_\", \" \"))) |&gt; \n  select(-c('state','Margin')) \nnames(df_2024)[2] &lt;- 'Donald.TrumpRepublican2024'\nnames(df_2024)[3] &lt;- 'Kamala.HarrisDemocratic2024'\nnames(df_2024)[4] &lt;- 'Total2024'\n\n\n\nfiles_2020 &lt;- list.files(path = \"data/mp04/States/\", pattern = \"2020_dataframe.csv\", full.names = TRUE)\ndf_2020 &lt;- read_csv(files_2020) |&gt; \n  bind_rows() |&gt; \n  filter(!`County`=='Totals') |&gt;\n  filter(!`County`=='Total') |&gt;\n  select(-c(`...1`))  |&gt; \n  mutate(State =  case_when(state=='Washington_(state)' ~ 'Washington',\n                              TRUE  ~ str_replace_all(state,\"_\", \" \"))) |&gt; \n  select(-c('state','Margin'))\nnames(df_2020)[2] &lt;- 'Donald.TrumpRepublican2020'\nnames(df_2020)[3] &lt;- 'Joe.BidenDemocratic2020'\nnames(df_2020)[4] &lt;- 'Total2020'\n\n\ntmp &lt;- left_join(df_2020,df_2024, by=join_by(State==State,County==County))\n\nresults &lt;- left_join(tmp,geo_shifted, by=join_by(State==STATE_NAME,County==NAME))"
  },
  {
    "objectID": "mp04.html#which-county-or-counties-cast-the-most-votes-for-trump-in-absolute-terms-in-2024",
    "href": "mp04.html#which-county-or-counties-cast-the-most-votes-for-trump-in-absolute-terms-in-2024",
    "title": "Mini Project 4",
    "section": "Which county or counties cast the most votes for Trump (in absolute terms) in 2024?",
    "text": "Which county or counties cast the most votes for Trump (in absolute terms) in 2024?\n\n\nCode\nresults |&gt; \n  slice_max(Donald.TrumpRepublican2024,n=1) |&gt;\n  select(State,County, Donald.TrumpRepublican2020) |&gt;\n  rename(`Votes for Donald Trump`=Donald.TrumpRepublican2020)|&gt; \n  gt() |&gt; \n  fmt_number(`Votes for Donald Trump`,decimals = 0)\n\n\n\n\n\n\n\n\nState\nCounty\nVotes for Donald Trump\n\n\n\n\nCalifornia\nLos Angeles\n1,145,530\n\n\n\n\n\n\n\nLos Angeles County had the largest absolute number of votes for Donald Trump in 2024 at 1,145,530 votes."
  },
  {
    "objectID": "mp04.html#which-county-or-counties-cast-the-most-votes-for-biden-as-a-fraction-of-total-votes-cast-in-2020",
    "href": "mp04.html#which-county-or-counties-cast-the-most-votes-for-biden-as-a-fraction-of-total-votes-cast-in-2020",
    "title": "Mini Project 4",
    "section": "Which county or counties cast the most votes for Biden (as a fraction of total votes cast) in 2020?",
    "text": "Which county or counties cast the most votes for Biden (as a fraction of total votes cast) in 2020?\n\n\nCode\nresults |&gt; \n  mutate(vote_percentage= Joe.BidenDemocratic2020/Total2020) |&gt;\n  select(State,County,vote_percentage) |&gt; \n  slice_max(vote_percentage, n=1) |&gt; \n  rename(`Percentage of Votes for Joe Biden`=vote_percentage) |&gt;\n  gt() |&gt; \n  fmt_percent(`Percentage of Votes for Joe Biden`)\n\n\n\n\n\n\n\n\nState\nCounty\nPercentage of Votes for Joe Biden\n\n\n\n\nHawaii\nKalawao\n95.83%\n\n\n\n\n\n\n\nKalawao County in Hawaii has the largest total percentage vote for Joe Biden in 2024 at 95.83% of the vote."
  },
  {
    "objectID": "mp04.html#which-county-or-counties-had-the-largest-shift-towards-trump-in-absolute-terms-in-2024",
    "href": "mp04.html#which-county-or-counties-had-the-largest-shift-towards-trump-in-absolute-terms-in-2024",
    "title": "Mini Project 4",
    "section": "Which county or counties had the largest shift towards Trump (in absolute terms) in 2024?",
    "text": "Which county or counties had the largest shift towards Trump (in absolute terms) in 2024?\n\n\nCode\nresults |&gt; \n  mutate(trump_shift= Donald.TrumpRepublican2024 - Donald.TrumpRepublican2020) |&gt;\n  select(State,County,trump_shift) |&gt; \n  slice_max(trump_shift, n=1) |&gt; \n  rename(`Increase in Trump Votes` = trump_shift) |&gt; \n  gt() |&gt; \n  fmt_number(`Increase in Trump Votes`,decimal=0)\n\n\n\n\n\n\n\n\nState\nCounty\nIncrease in Trump Votes\n\n\n\n\nFlorida\nMiami-Dade\n72,757\n\n\n\n\n\n\n\nMiami-Dade county had the largest absolute increase in Trump votes at 72,757 more Republican votes in 2024 than in 2020."
  },
  {
    "objectID": "mp04.html#which-state-had-the-largest-shift-towards-harris-or-smallest-shift-towards-trump-in-2024",
    "href": "mp04.html#which-state-had-the-largest-shift-towards-harris-or-smallest-shift-towards-trump-in-2024",
    "title": "Mini Project 4",
    "section": "Which state had the largest shift towards Harris (or smallest shift towards Trump) in 2024?",
    "text": "Which state had the largest shift towards Harris (or smallest shift towards Trump) in 2024?\n\n\nCode\nresults |&gt; \n  group_by(State) |&gt;\n  summarise(kamala_votes = sum(Kamala.HarrisDemocratic2024),\n            joe_votes = sum(Joe.BidenDemocratic2020)) |&gt; \n  mutate(dem_shift= kamala_votes - joe_votes) |&gt;\n  select(State,dem_shift) |&gt; \n  slice_max(dem_shift, n=1) |&gt;\n  rename(`Democratic Shift`=dem_shift) |&gt;\n  gt() |&gt; \n  fmt_number(`Democratic Shift`,decimal=0)\n\n\n\n\n\n\n\n\nState\nDemocratic Shift\n\n\n\n\nGeorgia\n74,384\n\n\n\n\n\n\n\nGeorgia had the largest gain in Democratic voters, with a total vote gain at 74,384 votes."
  },
  {
    "objectID": "mp04.html#what-is-the-largest-county-by-area-in-this-data-set",
    "href": "mp04.html#what-is-the-largest-county-by-area-in-this-data-set",
    "title": "Mini Project 4",
    "section": "What is the largest county, by area, in this data set?",
    "text": "What is the largest county, by area, in this data set?\n\n\nCode\nresults |&gt; \n  select(State,County,ALAND) |&gt;\n  slice_max(ALAND,n=1) |&gt;\n  rename(`Area (Square Meters)`=ALAND) |&gt; \n  gt() |&gt;\n  fmt_number(`Area (Square Meters)`,decimal=0)\n\n\n\n\n\n\n\n\nState\nCounty\nArea (Square Meters)\n\n\n\n\nCalifornia\nSan Bernardino\n51,976,311,343\n\n\n\n\n\n\n\nSan Bernardino county has the largest area at 51.98 Billion square meters."
  },
  {
    "objectID": "mp04.html#which-county-has-the-highest-voter-density-voters-per-unit-of-area-in-2020",
    "href": "mp04.html#which-county-has-the-highest-voter-density-voters-per-unit-of-area-in-2020",
    "title": "Mini Project 4",
    "section": "Which county has the highest voter density (voters per unit of area) in 2020?",
    "text": "Which county has the highest voter density (voters per unit of area) in 2020?\n\n\nCode\n results |&gt; \n  select(State,County,Total2020,ALAND) |&gt;\n  mutate(Density2020=Total2020/ALAND) |&gt; \n  slice_max(Density2020,n=1) |&gt; \n  rename(`Total Votes in 2020`=Total2020,\n         `Area (Square Meters)`=ALAND,\n         `Number of Voters per Square Meter`=Density2020)|&gt;\n  gt() |&gt; \n  fmt_number(c(`Total Votes in 2020`,\n         `Area (Square Meters)`), decimal=0)\n\n\n\n\n\n\n\n\nState\nCounty\nTotal Votes in 2020\nArea (Square Meters)\nNumber of Voters per Square Meter\n\n\n\n\nVirginia\nFairfax\n600,823\n16,163,924\n0.03717062\n\n\n\n\n\n\n\nFairfax county in Virginia had the largest voter density at 0.037 voters per square meter."
  },
  {
    "objectID": "mp04.html#which-county-had-the-largest-increase-in-voter-turnout-in-2024",
    "href": "mp04.html#which-county-had-the-largest-increase-in-voter-turnout-in-2024",
    "title": "Mini Project 4",
    "section": "Which county had the largest increase in voter turnout in 2024?",
    "text": "Which county had the largest increase in voter turnout in 2024?\n\n\nCode\n results |&gt; \n  select(State,County,Total2024,Total2020) |&gt;\n  mutate(Change=Total2024-Total2020) |&gt; \n  slice_max(Change,n=1) |&gt;\n  rename(`Voter Count 2024`=Total2024,\n         `Voter Count 2020`=Total2020) |&gt;\n  gt() |&gt; \n  fmt_number(c(`Voter Count 2024`,`Voter Count 2020`),decimals = 0)\n\n\n\n\n\n\n\n\nState\nCounty\nVoter Count 2024\nVoter Count 2020\nChange\n\n\n\n\nNevada\nClark\n1,031,223\n972,510\n58713\n\n\n\n\n\n\n\nClark county in Nevada had the largest increase in voter turn out with 58,713 more individual votes going to the polls in 2024 compared to 2020."
  }
]